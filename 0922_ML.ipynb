{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래 경로의 폴더 안에 있는 파일들에 대해서 ML models 실험.\n",
    "경로 : \"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data\"\n",
    "\n",
    "- labeling 기준 : eGFR < 60 기준 만을 사용. => tight3.csv 파일 사용.\n",
    "- original vs under-sampling vs over-sampling\n",
    "    - (basic) vs (food feature) vs (basic + food feature)\n",
    "- SVM, RF, GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from scipy import stats\n",
    "from scipy.stats import randint, loguniform\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed = 1109\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test에 나머지 control sample 추가해서 idx 만 반환\n",
    "def divide_testset(unbalanced_data, ratio):\n",
    "    # train에서 ckd, control index 확인\n",
    "    total_idx = unbalanced_data.index\n",
    "    ckd_idx = unbalanced_data[unbalanced_data['onset_tight'] == 1].index        # 실제 ckd\n",
    "    control_idx = unbalanced_data[unbalanced_data['onset_tight'] == 0].index    # 실제 control\n",
    "    # print(control_idx, ckd_idx)\n",
    "\n",
    "    # ckd 갯수와 동일하게 control idx sampling\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    sampled_ckd_idx = pd.Index(rng.choice(ckd_idx, size=int(len(ckd_idx)*ratio), replace=False))\n",
    "    sampled_control_idx = pd.Index(rng.choice(control_idx, size=len(sampled_ckd_idx), replace=False)) # test_ckd 갯수와 동일하게 sampling\n",
    "    \n",
    "    test_idx = sampled_ckd_idx.append(sampled_control_idx)\n",
    "    train_idx = total_idx.difference(test_idx)\n",
    "\n",
    "    # return 실제 ckd, 실제 ckd 갯수와 동일한 갯수의 subject, control_idx - ckd_idx\n",
    "    return unbalanced_data.loc[train_idx], unbalanced_data.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Oversampling\n",
    "def oversampling(unbalanced_dataframe):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote = SMOTE(random_state=seed)\n",
    "    temp = unbalanced_dataframe.drop(['RID'], axis=1)\n",
    "    X_train, y_train = smote.fit_resample(temp, temp['onset_tight'])\n",
    "\n",
    "    # X_train에는 RID, onset_3 없음.\n",
    "    return X_train.drop(['onset_tight'], axis=1), y_train\n",
    "\n",
    "### Undersampling\n",
    "# test에 나머지 control sample 추가해서 idx 만 반환\n",
    "def _under_sampling_idx(unbalanced_data):\n",
    "    # train에서 ckd, control index 확인\n",
    "    ckd_idx = unbalanced_data[unbalanced_data['onset_tight'] == 1].index        # 실제 ckd\n",
    "    control_idx = unbalanced_data[unbalanced_data['onset_tight'] == 0].index    # 실제 control\n",
    "    # print(control_idx, ckd_idx)\n",
    "\n",
    "    # ckd 갯수와 동일하게 control idx sampling\n",
    "    sampled_control_idx = pd.Index(np.random.choice(control_idx, size=len(ckd_idx), replace=False)) # ckd 갯수와 동일하게 sampling\n",
    "    not_sampled_control_idx = control_idx.difference(sampled_control_idx)\n",
    "\n",
    "    # 잘 sampling 되었는지 확인\n",
    "    assert set(sampled_control_idx).issubset(set(control_idx))\n",
    "    # print(len(sampled_control_idx))\n",
    "\n",
    "    balanced_idx = sampled_control_idx.append(ckd_idx)\n",
    "\n",
    "    # return 실제 ckd, 실제 ckd 갯수와 동일한 갯수의 subject, control_idx - ckd_idx\n",
    "    return ckd_idx, sampled_control_idx, not_sampled_control_idx, balanced_idx\n",
    "\n",
    "def undersampling(unbalanced_data):\n",
    "    a, b, c, d = _under_sampling_idx(unbalanced_data)\n",
    "    under_sampled_data = unbalanced_data.loc[d]\n",
    "    X_undersampled = under_sampled_data.drop(['RID', 'onset_tight'], axis=1)\n",
    "    y_undersampled = under_sampled_data['onset_tight']\n",
    "    return X_undersampled, y_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_results(y_test, final_prediction):\n",
    "#     cm = confusion_matrix(list(y_test), list(final_prediction))\n",
    "#     print(cm)\n",
    "#     tn, fn, tp, fp  = cm[0][0], cm[1][0], cm[1][1], cm[0][1]\n",
    "#     recall = tp / (fn + tp)\n",
    "#     precision = tp / (fp + tp)\n",
    "#     acc = (tp + tn) / (tn + fn + tp+fp)\n",
    "#     print(\"Recall \\t Precision \\t Acc\")\n",
    "#     print(f\"{np.round(recall,4)} {np.round(precision,4)} {np.round(acc,4)}\")\n",
    "\n",
    "def get_results(y_test, final_prediction, final_probabilities):\n",
    "    cm = confusion_matrix(list(y_test), list(final_prediction))\n",
    "    print(cm)\n",
    "    tn, fn, tp, fp  = cm[0][0], cm[1][0], cm[1][1], cm[0][1]\n",
    "    recall = tp / (fn + tp)\n",
    "    precision = tp / (fp + tp)\n",
    "    acc = (tp + tn) / (tn + fn + tp + fp)\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, final_probabilities)\n",
    "    \n",
    "    print(\"Recall \\t Precision \\t Acc \\t AUC\")\n",
    "    print(f\"{np.round(recall, 4)} {np.round(precision, 4)} {np.round(acc, 4)} {np.round(auc, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fitting(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def model_eval(fitted_model, X_test, y_test):\n",
    "    model_prediction = fitted_model.predict(X_test)\n",
    "    model_probabilities = fitted_model.predict_proba(X_test)[:, 1]\n",
    "    model_score = fitted_model.score(X_test, y_test)\n",
    "    print(f\"Score with simple {fitted_model} model\")\n",
    "    print(0.5, np.round(model_score, 4))     # accuracy\n",
    "\n",
    "    get_results(y_test, model_prediction, model_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Food sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum = pd.read_csv(\"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data/0922_basic_food_sum.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RID', 'CT1_SEX', 'CT1_AGE', 'CT1_eGFR', 'imp_CT1_SBP', 'imp_CT1_BMI',\n",
       "       'CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL', 'imp_new_EDU',\n",
       "       'imp_new_INCOME', 'imp_new_DRINK', 'imp_new_SMOKE', 'new_CT1_gohyeol',\n",
       "       'CT1_dangnyo', 'CT1_simhyeol', 'onset_tight', 'F01_Sum', 'F02_Sum',\n",
       "       'F03_Sum', 'F04_Sum', 'F05_Sum', 'F06_Sum', 'F07_Sum', 'F08_Sum',\n",
       "       'F09_Sum', 'F10_Sum', 'F11_Sum', 'F12_Sum', 'F13_Sum', 'F14_Sum',\n",
       "       'F15_Sum', 'F16_Sum', 'F17_Sum', 'F18_Sum', 'F19_Sum', 'F20_Sum',\n",
       "       'F21_Sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tight3_foodsum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum_basic = tight3_foodsum[['RID', 'CT1_SEX', 'CT1_AGE', 'CT1_eGFR', 'imp_CT1_SBP', 'imp_CT1_BMI',\n",
    "       'CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL', 'imp_new_EDU',\n",
    "       'imp_new_INCOME', 'imp_new_DRINK', 'imp_new_SMOKE', 'new_CT1_gohyeol',\n",
    "       'CT1_dangnyo', 'CT1_simhyeol', 'onset_tight']]\n",
    "tight3_foodsum_food = tight3_foodsum[['RID', 'onset_tight', 'F01_Sum', 'F02_Sum',\n",
    "       'F03_Sum', 'F04_Sum', 'F05_Sum', 'F06_Sum', 'F07_Sum', 'F08_Sum',\n",
    "       'F09_Sum', 'F10_Sum', 'F11_Sum', 'F12_Sum', 'F13_Sum', 'F14_Sum',\n",
    "       'F15_Sum', 'F16_Sum', 'F17_Sum', 'F18_Sum', 'F19_Sum', 'F20_Sum',\n",
    "       'F21_Sum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Original vs Under-sampling vs Over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum_train, tight3_foodsum_test = divide_testset(tight3_foodsum, 0.1)        # 기본 정보만 가진 데이터\n",
    "\n",
    "y_tight3_test = tight3_foodsum_test['onset_tight']\n",
    "X_tight3_test = tight3_foodsum_test.drop(['RID', 'onset_tight'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original (57127, 36) (57127,) onset_tight\n",
      "0    56654\n",
      "1      473\n",
      "Name: count, dtype: int64\n",
      "undersampling (946, 36) (946,) onset_tight\n",
      "0    473\n",
      "1    473\n",
      "Name: count, dtype: int64\n",
      "oversampling (113308, 36) (113308,) onset_tight\n",
      "0    56654\n",
      "1    56654\n",
      "Name: count, dtype: int64\n",
      "test (104, 36) (104,) onset_tight\n",
      "1    52\n",
      "0    52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## key : dataset name\n",
    "## value : (train dataset, train label)\n",
    "datas = {\"original\":(), \"undersampling\":(), \"oversampling\":(), \"test\":(X_tight3_test, y_tight3_test)}\n",
    "\n",
    "datas['undersampling'] = undersampling(tight3_foodsum_train)\n",
    "datas['oversampling'] = oversampling(tight3_foodsum_train)\n",
    "\n",
    "y_tight3_foodsum_train = tight3_foodsum_train['onset_tight']\n",
    "X_tight3_foodsum_train = tight3_foodsum_train.drop(['RID', 'onset_tight'], axis=1)\n",
    "datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "\n",
    "# !!!data shape check!!!\n",
    "for k, v in datas.items():\n",
    "    print(k, v[0].shape, v[1].shape, v[1].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Eval\n",
    "- svm, Decision Tree, logistic regression, linear regression, Gradient Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n",
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.8654\n",
      "[[41 11]\n",
      " [ 3 49]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9423 0.8167 0.8654 0.9393\n",
      "\n",
      "!!! RF !!!\n",
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.8654\n",
      "[[42 10]\n",
      " [ 4 48]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9231 0.8276 0.8654 0.9153\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.8846\n",
      "[[42 10]\n",
      " [ 2 50]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9615 0.8333 0.8846 0.939\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.8269\n",
      "[[37 15]\n",
      " [ 3 49]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9423 0.7656 0.8269 0.9057\n",
      "For original dataset!!!\n",
      "original dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n",
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.7596\n",
      "\n",
      "!!! RF !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.9271\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.5096\n",
      "[[52  0]\n",
      " [51  1]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0192 1.0 0.5096 0.9427\n",
      "\n",
      "!!! GBC !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.8719\n",
      "For oversampling dataset!!!\n",
      "oversampling dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.7788\n",
      "[[46  6]\n",
      " [17 35]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.6731 0.8537 0.7788 0.909\n",
      "\n",
      "!!! RF !!!\n",
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.7596\n",
      "[[42 10]\n",
      " [15 37]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.7115 0.7872 0.7596 0.882\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.7692\n",
      "[[46  6]\n",
      " [18 34]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.6538 0.85 0.7692 0.9087\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.8365\n",
      "[[40 12]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.7966 0.8365 0.8794\n"
     ]
    }
   ],
   "source": [
    "# 159m 37.5s\n",
    "for data in ['undersampling', 'original', 'oversampling']:\n",
    "    print(f\"For {data} dataset!!!\")\n",
    "    X_test, y_test = datas['test']\n",
    "    X_train, y_train = datas[data]\n",
    "    \n",
    "    wei_train_scaler = StandardScaler()\n",
    "    X_train = wei_train_scaler.fit_transform(X_train)\n",
    "    X_test = wei_train_scaler.transform(X_test)\n",
    "    print(f\"{data} dataset loaded and scaled\")\n",
    "    \n",
    "    ### Train and Eval per each Model ###\n",
    "    print()\n",
    "    print(\"!!! SVM !!!\")\n",
    "    svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "    svm_model = model_fitting(svm_model, X_train, y_train)\n",
    "    model_eval(svm_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! RF !!!\")\n",
    "    rf_model = RandomForestClassifier(random_state=seed, max_depth=3)\n",
    "    rf_model = model_fitting(rf_model, X_train, y_train)\n",
    "    model_eval(rf_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! LR !!!\")\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    lr_model = model_fitting(lr_model, X_train, y_train)\n",
    "    model_eval(lr_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! GBC !!!\")\n",
    "    gbc_model = GradientBoostingClassifier(random_state=seed, max_depth=1, learning_rate=0.01)\n",
    "    gbc_model = model_fitting(gbc_model, X_train, y_train)\n",
    "    model_eval(gbc_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Food mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum = pd.read_csv(\"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data/0922_basic_food_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RID', 'CT1_SEX', 'CT1_AGE', 'CT1_eGFR', 'imp_CT1_SBP', 'imp_CT1_BMI',\n",
       "       'CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL', 'imp_new_EDU',\n",
       "       'imp_new_INCOME', 'imp_new_DRINK', 'imp_new_SMOKE', 'new_CT1_gohyeol',\n",
       "       'CT1_dangnyo', 'CT1_simhyeol', 'onset_tight', 'F01_Mean', 'F02_Mean',\n",
       "       'F03_Mean', 'F04_Mean', 'F05_Mean', 'F06_Mean', 'F07_Mean', 'F08_Mean',\n",
       "       'F09_Mean', 'F10_Mean', 'F11_Mean', 'F12_Mean', 'F13_Mean', 'F14_Mean',\n",
       "       'F15_Mean', 'F16_Mean', 'F17_Mean', 'F18_Mean', 'F19_Mean', 'F20_Mean',\n",
       "       'F21_Mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tight3_foodsum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum_basic = tight3_foodsum[['RID', 'CT1_SEX', 'CT1_AGE', 'CT1_eGFR', 'imp_CT1_SBP', 'imp_CT1_BMI',\n",
    "       'CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL', 'imp_new_EDU',\n",
    "       'imp_new_INCOME', 'imp_new_DRINK', 'imp_new_SMOKE', 'new_CT1_gohyeol',\n",
    "       'CT1_dangnyo', 'CT1_simhyeol', 'onset_tight']]\n",
    "tight3_foodsum_food = tight3_foodsum[['RID', 'onset_tight', 'F01_Mean', 'F02_Mean', 'F03_Mean',\n",
    "       'F04_Mean', 'F05_Mean', 'F06_Mean', 'F07_Mean', 'F08_Mean', 'F09_Mean',\n",
    "       'F10_Mean', 'F11_Mean', 'F12_Mean', 'F13_Mean', 'F14_Mean', 'F15_Mean',\n",
    "       'F16_Mean', 'F17_Mean', 'F18_Mean', 'F19_Mean', 'F20_Mean', 'F21_Mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Original vs Under-sampling vs Over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum_train, tight3_foodsum_test = divide_testset(tight3_foodsum, 0.1)        # 기본 정보만 가진 데이터\n",
    "\n",
    "y_tight3_test = tight3_foodsum_test['onset_tight']\n",
    "X_tight3_test = tight3_foodsum_test.drop(['RID', 'onset_tight'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original (57127, 36) (57127,) onset_tight\n",
      "0    56654\n",
      "1      473\n",
      "Name: count, dtype: int64\n",
      "undersampling (946, 36) (946,) onset_tight\n",
      "0    473\n",
      "1    473\n",
      "Name: count, dtype: int64\n",
      "oversampling (113308, 36) (113308,) onset_tight\n",
      "0    56654\n",
      "1    56654\n",
      "Name: count, dtype: int64\n",
      "test (104, 36) (104,) onset_tight\n",
      "1    52\n",
      "0    52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## key : dataset name\n",
    "## value : (train dataset, train label)\n",
    "datas = {\"original\":(), \"undersampling\":(), \"oversampling\":(), \"test\":(X_tight3_test, y_tight3_test)}\n",
    "\n",
    "datas['undersampling'] = undersampling(tight3_foodsum_train)\n",
    "datas['oversampling'] = oversampling(tight3_foodsum_train)\n",
    "\n",
    "y_tight3_foodsum_train = tight3_foodsum_train['onset_tight']\n",
    "X_tight3_foodsum_train = tight3_foodsum_train.drop(['RID', 'onset_tight'], axis=1)\n",
    "datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "\n",
    "# !!!data shape check!!!\n",
    "for k, v in datas.items():\n",
    "    print(k, v[0].shape, v[1].shape, v[1].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Eval\n",
    "- svm, Decision Tree, logistic regression, linear regression, Gradient Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n",
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.875\n",
      "[[42 10]\n",
      " [ 3 49]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9423 0.8305 0.875 0.9419\n",
      "\n",
      "!!! RF !!!\n",
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.875\n",
      "[[43  9]\n",
      " [ 4 48]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9231 0.8421 0.875 0.912\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.8846\n",
      "[[43  9]\n",
      " [ 3 49]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9423 0.8448 0.8846 0.9323\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.8654\n",
      "[[43  9]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.8393 0.8654 0.8963\n",
      "For original dataset!!!\n",
      "original dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n",
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.7596\n",
      "\n",
      "!!! RF !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.9271\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.5096\n",
      "[[52  0]\n",
      " [51  1]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0192 1.0 0.5096 0.9427\n",
      "\n",
      "!!! GBC !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.8719\n",
      "For oversampling dataset!!!\n",
      "oversampling dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.7788\n",
      "[[45  7]\n",
      " [16 36]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.6923 0.8372 0.7788 0.9094\n",
      "\n",
      "!!! RF !!!\n",
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.7596\n",
      "[[42 10]\n",
      " [15 37]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.7115 0.7872 0.7596 0.8857\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.7885\n",
      "[[46  6]\n",
      " [16 36]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.6923 0.8571 0.7885 0.9105\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.8462\n",
      "[[41 11]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.8103 0.8462 0.8955\n"
     ]
    }
   ],
   "source": [
    "# 197m 37.5s\n",
    "for data in ['undersampling', 'original', 'oversampling']:\n",
    "    print(f\"For {data} dataset!!!\")\n",
    "    X_test, y_test = datas['test']\n",
    "    X_train, y_train = datas[data]\n",
    "    \n",
    "    wei_train_scaler = StandardScaler()\n",
    "    X_train = wei_train_scaler.fit_transform(X_train)\n",
    "    X_test = wei_train_scaler.transform(X_test)\n",
    "    print(f\"{data} dataset loaded and scaled\")\n",
    "    \n",
    "    ### Train and Eval per each Model ###\n",
    "    print()\n",
    "    print(\"!!! SVM !!!\")\n",
    "    svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "    svm_model = model_fitting(svm_model, X_train, y_train)\n",
    "    model_eval(svm_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! RF !!!\")\n",
    "    rf_model = RandomForestClassifier(random_state=seed, max_depth=3)\n",
    "    rf_model = model_fitting(rf_model, X_train, y_train)\n",
    "    model_eval(rf_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! LR !!!\")\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    lr_model = model_fitting(lr_model, X_train, y_train)\n",
    "    model_eval(lr_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! GBC !!!\")\n",
    "    gbc_model = GradientBoostingClassifier(random_state=seed, max_depth=1, learning_rate=0.01)\n",
    "    gbc_model = model_fitting(gbc_model, X_train, y_train)\n",
    "    model_eval(gbc_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Food adjsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum = pd.read_csv(\"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data/0922_basic_food_adjusted_sum.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RID', 'CT1_SEX', 'CT1_AGE', 'CT1_eGFR', 'imp_CT1_SBP', 'imp_CT1_BMI',\n",
       "       'CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL', 'imp_new_EDU',\n",
       "       'imp_new_INCOME', 'imp_new_DRINK', 'imp_new_SMOKE', 'new_CT1_gohyeol',\n",
       "       'CT1_dangnyo', 'CT1_simhyeol', 'onset_tight', 'F01_Sum_res',\n",
       "       'F02_Sum_res', 'F03_Sum_res', 'F04_Sum_res', 'F05_Sum_res',\n",
       "       'F06_Sum_res', 'F07_Sum_res', 'F08_Sum_res', 'F09_Sum_res',\n",
       "       'F10_Sum_res', 'F11_Sum_res', 'F12_Sum_res', 'F13_Sum_res',\n",
       "       'F14_Sum_res', 'F15_Sum_res', 'F16_Sum_res', 'F17_Sum_res',\n",
       "       'F18_Sum_res', 'F19_Sum_res', 'F20_Sum_res', 'F21_Sum_res'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tight3_foodsum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum_basic = tight3_foodsum[['RID', 'CT1_SEX', 'CT1_AGE', 'CT1_eGFR', 'imp_CT1_SBP', 'imp_CT1_BMI',\n",
    "       'CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL', 'imp_new_EDU',\n",
    "       'imp_new_INCOME', 'imp_new_DRINK', 'imp_new_SMOKE', 'new_CT1_gohyeol',\n",
    "       'CT1_dangnyo', 'CT1_simhyeol', 'onset_tight']]\n",
    "tight3_foodsum_food = tight3_foodsum[['RID', 'onset_tight', 'F01_Sum_res', 'F02_Sum_res', 'F03_Sum_res', 'F04_Sum_res',\n",
    "       'F05_Sum_res', 'F06_Sum_res', 'F07_Sum_res', 'F08_Sum_res',\n",
    "       'F09_Sum_res', 'F10_Sum_res', 'F11_Sum_res', 'F12_Sum_res',\n",
    "       'F13_Sum_res', 'F14_Sum_res', 'F15_Sum_res', 'F16_Sum_res',\n",
    "       'F17_Sum_res', 'F18_Sum_res', 'F19_Sum_res', 'F20_Sum_res',\n",
    "       'F21_Sum_res']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Original vs Under-sampling vs Over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum_train, tight3_foodsum_test = divide_testset(tight3_foodsum, 0.1)        # 기본 정보만 가진 데이터\n",
    "\n",
    "y_tight3_test = tight3_foodsum_test['onset_tight']\n",
    "X_tight3_test = tight3_foodsum_test.drop(['RID', 'onset_tight'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original (57127, 36) (57127,) onset_tight\n",
      "0    56654\n",
      "1      473\n",
      "Name: count, dtype: int64\n",
      "undersampling (946, 36) (946,) onset_tight\n",
      "0    473\n",
      "1    473\n",
      "Name: count, dtype: int64\n",
      "oversampling (113308, 36) (113308,) onset_tight\n",
      "0    56654\n",
      "1    56654\n",
      "Name: count, dtype: int64\n",
      "test (104, 36) (104,) onset_tight\n",
      "1    52\n",
      "0    52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## key : dataset name\n",
    "## value : (train dataset, train label)\n",
    "datas = {\"original\":(), \"undersampling\":(), \"oversampling\":(), \"test\":(X_tight3_test, y_tight3_test)}\n",
    "\n",
    "datas['undersampling'] = undersampling(tight3_foodsum_train)\n",
    "datas['oversampling'] = oversampling(tight3_foodsum_train)\n",
    "\n",
    "y_tight3_foodsum_train = tight3_foodsum_train['onset_tight']\n",
    "X_tight3_foodsum_train = tight3_foodsum_train.drop(['RID', 'onset_tight'], axis=1)\n",
    "datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "\n",
    "# !!!data shape check!!!\n",
    "for k, v in datas.items():\n",
    "    print(k, v[0].shape, v[1].shape, v[1].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Eval\n",
    "- svm, Decision Tree, logistic regression, linear regression, Gradient Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n",
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.875\n",
      "[[42 10]\n",
      " [ 3 49]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9423 0.8305 0.875 0.9316\n",
      "\n",
      "!!! RF !!!\n",
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.875\n",
      "[[44  8]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.8545 0.875 0.9046\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.8654\n",
      "[[42 10]\n",
      " [ 4 48]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9231 0.8276 0.8654 0.9331\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.8462\n",
      "[[41 11]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.8103 0.8462 0.9136\n",
      "For original dataset!!!\n",
      "original dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n",
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.8325\n",
      "\n",
      "!!! RF !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.9294\n",
      "\n",
      "!!! LR !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.5096\n",
      "[[52  0]\n",
      " [51  1]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0192 1.0 0.5096 0.9427\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.8719\n",
      "For oversampling dataset!!!\n",
      "oversampling dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.8077\n",
      "[[46  6]\n",
      " [14 38]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.7308 0.8636 0.8077 0.9079\n",
      "\n",
      "!!! RF !!!\n",
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.8269\n",
      "[[43  9]\n",
      " [ 9 43]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.8269 0.8269 0.8269 0.8828\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.7981\n",
      "[[46  6]\n",
      " [15 37]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.7115 0.8605 0.7981 0.9094\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.8365\n",
      "[[40 12]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.7966 0.8365 0.8996\n"
     ]
    }
   ],
   "source": [
    "# 222m\n",
    "for data in ['undersampling', 'original', 'oversampling']:\n",
    "    print(f\"For {data} dataset!!!\")\n",
    "    X_test, y_test = datas['test']\n",
    "    X_train, y_train = datas[data]\n",
    "    \n",
    "    wei_train_scaler = StandardScaler()\n",
    "    X_train = wei_train_scaler.fit_transform(X_train)\n",
    "    X_test = wei_train_scaler.transform(X_test)\n",
    "    print(f\"{data} dataset loaded and scaled\")\n",
    "    \n",
    "    ### Train and Eval per each Model ###\n",
    "    print()\n",
    "    print(\"!!! SVM !!!\")\n",
    "    svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "    svm_model = model_fitting(svm_model, X_train, y_train)\n",
    "    model_eval(svm_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! RF !!!\")\n",
    "    rf_model = RandomForestClassifier(random_state=seed, max_depth=3)\n",
    "    rf_model = model_fitting(rf_model, X_train, y_train)\n",
    "    model_eval(rf_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! LR !!!\")\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    lr_model = model_fitting(lr_model, X_train, y_train)\n",
    "    model_eval(lr_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! GBC !!!\")\n",
    "    gbc_model = GradientBoostingClassifier(random_state=seed, max_depth=1, learning_rate=0.01)\n",
    "    gbc_model = model_fitting(gbc_model, X_train, y_train)\n",
    "    model_eval(gbc_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Food adj mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum = pd.read_csv(\"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data/0922_basic_food_adjusted_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RID', 'CT1_SEX', 'CT1_AGE', 'CT1_eGFR', 'imp_CT1_SBP', 'imp_CT1_BMI',\n",
       "       'CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL', 'imp_new_EDU',\n",
       "       'imp_new_INCOME', 'imp_new_DRINK', 'imp_new_SMOKE', 'new_CT1_gohyeol',\n",
       "       'CT1_dangnyo', 'CT1_simhyeol', 'onset_tight', 'F01_Mean_res',\n",
       "       'F02_Mean_res', 'F03_Mean_res', 'F04_Mean_res', 'F05_Mean_res',\n",
       "       'F06_Mean_res', 'F07_Mean_res', 'F08_Mean_res', 'F09_Mean_res',\n",
       "       'F10_Mean_res', 'F11_Mean_res', 'F12_Mean_res', 'F13_Mean_res',\n",
       "       'F14_Mean_res', 'F15_Mean_res', 'F16_Mean_res', 'F17_Mean_res',\n",
       "       'F18_Mean_res', 'F19_Mean_res', 'F20_Mean_res', 'F21_Mean_res'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tight3_foodsum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum_basic = tight3_foodsum[['RID', 'CT1_SEX', 'CT1_AGE', 'CT1_eGFR', 'imp_CT1_SBP', 'imp_CT1_BMI',\n",
    "       'CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL', 'imp_new_EDU',\n",
    "       'imp_new_INCOME', 'imp_new_DRINK', 'imp_new_SMOKE', 'new_CT1_gohyeol',\n",
    "       'CT1_dangnyo', 'CT1_simhyeol', 'onset_tight']]\n",
    "tight3_foodsum_food = tight3_foodsum[['RID', 'onset_tight', 'F01_Mean_res', 'F02_Mean_res', 'F03_Mean_res',\n",
    "       'F04_Mean_res', 'F05_Mean_res', 'F06_Mean_res', 'F07_Mean_res',\n",
    "       'F08_Mean_res', 'F09_Mean_res', 'F10_Mean_res', 'F11_Mean_res',\n",
    "       'F12_Mean_res', 'F13_Mean_res', 'F14_Mean_res', 'F15_Mean_res',\n",
    "       'F16_Mean_res', 'F17_Mean_res', 'F18_Mean_res', 'F19_Mean_res',\n",
    "       'F20_Mean_res', 'F21_Mean_res']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Original vs Under-sampling vs Over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum_train, tight3_foodsum_test = divide_testset(tight3_foodsum, 0.1)        # 기본 정보만 가진 데이터\n",
    "\n",
    "y_tight3_test = tight3_foodsum_test['onset_tight']\n",
    "X_tight3_test = tight3_foodsum_test.drop(['RID', 'onset_tight'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original (57127, 36) (57127,) onset_tight\n",
      "0    56654\n",
      "1      473\n",
      "Name: count, dtype: int64\n",
      "undersampling (946, 36) (946,) onset_tight\n",
      "0    473\n",
      "1    473\n",
      "Name: count, dtype: int64\n",
      "oversampling (113308, 36) (113308,) onset_tight\n",
      "0    56654\n",
      "1    56654\n",
      "Name: count, dtype: int64\n",
      "test (104, 36) (104,) onset_tight\n",
      "1    52\n",
      "0    52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## key : dataset name\n",
    "## value : (train dataset, train label)\n",
    "datas = {\"original\":(), \"undersampling\":(), \"oversampling\":(), \"test\":(X_tight3_test, y_tight3_test)}\n",
    "\n",
    "datas['undersampling'] = undersampling(tight3_foodsum_train)\n",
    "datas['oversampling'] = oversampling(tight3_foodsum_train)\n",
    "\n",
    "y_tight3_foodsum_train = tight3_foodsum_train['onset_tight']\n",
    "X_tight3_foodsum_train = tight3_foodsum_train.drop(['RID', 'onset_tight'], axis=1)\n",
    "datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "\n",
    "# !!!data shape check!!!\n",
    "for k, v in datas.items():\n",
    "    print(k, v[0].shape, v[1].shape, v[1].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Eval\n",
    "- svm, Decision Tree, logistic regression, linear regression, Gradient Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n",
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.8654\n",
      "[[42 10]\n",
      " [ 4 48]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9231 0.8276 0.8654 0.9283\n",
      "\n",
      "!!! RF !!!\n",
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.8654\n",
      "[[43  9]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.8393 0.8654 0.9116\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.8558\n",
      "[[42 10]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.8246 0.8558 0.9283\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.8462\n",
      "[[41 11]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.8103 0.8462 0.912\n",
      "For original dataset!!!\n",
      "original dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n",
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.8328\n",
      "\n",
      "!!! RF !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.9294\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.5096\n",
      "[[52  0]\n",
      " [51  1]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0192 1.0 0.5096 0.9427\n",
      "\n",
      "!!! GBC !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.5\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.8719\n",
      "For oversampling dataset!!!\n",
      "oversampling dataset loaded and scaled\n",
      "\n",
      "!!! SVM !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538305/495735168.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (fp + tp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with simple SVC(kernel='linear', probability=True, random_state=42) model\n",
      "0.5 0.8269\n",
      "[[46  6]\n",
      " [12 40]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.7692 0.8696 0.8269 0.9153\n",
      "\n",
      "!!! RF !!!\n",
      "Score with simple RandomForestClassifier(max_depth=3, random_state=1109) model\n",
      "0.5 0.8365\n",
      "[[44  8]\n",
      " [ 9 43]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.8269 0.8431 0.8365 0.8946\n",
      "\n",
      "!!! LR !!!\n",
      "Score with simple LogisticRegression(max_iter=1000, random_state=1109) model\n",
      "0.5 0.8173\n",
      "[[46  6]\n",
      " [13 39]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.75 0.8667 0.8173 0.9157\n",
      "\n",
      "!!! GBC !!!\n",
      "Score with simple GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=1109) model\n",
      "0.5 0.8462\n",
      "[[41 11]\n",
      " [ 5 47]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.9038 0.8103 0.8462 0.9031\n"
     ]
    }
   ],
   "source": [
    "# 159m 37.5s\n",
    "for data in ['undersampling', 'original', 'oversampling']:\n",
    "    print(f\"For {data} dataset!!!\")\n",
    "    X_test, y_test = datas['test']\n",
    "    X_train, y_train = datas[data]\n",
    "    \n",
    "    wei_train_scaler = StandardScaler()\n",
    "    X_train = wei_train_scaler.fit_transform(X_train)\n",
    "    X_test = wei_train_scaler.transform(X_test)\n",
    "    print(f\"{data} dataset loaded and scaled\")\n",
    "    \n",
    "    ### Train and Eval per each Model ###\n",
    "    print()\n",
    "    print(\"!!! SVM !!!\")\n",
    "    svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "    svm_model = model_fitting(svm_model, X_train, y_train)\n",
    "    model_eval(svm_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! RF !!!\")\n",
    "    rf_model = RandomForestClassifier(random_state=seed, max_depth=3)\n",
    "    rf_model = model_fitting(rf_model, X_train, y_train)\n",
    "    model_eval(rf_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! LR !!!\")\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    lr_model = model_fitting(lr_model, X_train, y_train)\n",
    "    model_eval(lr_model, X_test, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"!!! GBC !!!\")\n",
    "    gbc_model = GradientBoostingClassifier(random_state=seed, max_depth=1, learning_rate=0.01)\n",
    "    gbc_model = model_fitting(gbc_model, X_train, y_train)\n",
    "    model_eval(gbc_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
