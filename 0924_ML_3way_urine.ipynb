{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래 경로의 폴더 안에 있는 파일들에 대해서 ML models 실험.\n",
    "경로 : \"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data\"\n",
    "\n",
    "- labeling 기준 : eGFR < 60 기준 만을 사용. => tight3.csv 파일 사용.\n",
    "- original vs under-sampling vs over-sampling\n",
    "    - (basic) vs (food feature) vs (basic + food feature)\n",
    "- SVM, RF, GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from scipy import stats\n",
    "from scipy.stats import randint, loguniform\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1109)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test에 나머지 control sample 추가해서 idx 만 반환\n",
    "def divide_testset(unbalanced_data, ratio):\n",
    "    # train에서 ckd, control index 확인\n",
    "    total_idx = unbalanced_data.index\n",
    "    ckd2_idx = unbalanced_data[unbalanced_data['onset_tight'] == 2].index        # eGFR > 60 & urineprotein > 1+\n",
    "    ckd1_idx = unbalanced_data[unbalanced_data['onset_tight'] == 1].index        # 실제 ckd\n",
    "    control_idx = unbalanced_data[unbalanced_data['onset_tight'] == 0].index    # 실제 control\n",
    "    # print(control_idx, ckd_idx)\n",
    "\n",
    "    # ckd 갯수와 동일하게 control idx sampling\n",
    "    rng = np.random.default_rng(seed=0) \n",
    "    sampled_ckd1_idx = pd.Index(rng.choice(ckd1_idx, size=int(len(ckd1_idx)*ratio), replace=False))\n",
    "    \n",
    "    sampled_control_idx = pd.Index(rng.choice(control_idx, size=len(sampled_ckd1_idx), replace=False)) # test_ckd 갯수와 control sampling\n",
    "    sampled_ckd2_idx = pd.Index(rng.choice(ckd2_idx, size=len(sampled_ckd1_idx), replace=False)) # test_ckd 갯수와 ckd2 sampling\n",
    "    \n",
    "    test_idx = sampled_ckd1_idx.append(sampled_control_idx)\n",
    "    test_idx = test_idx.append(sampled_ckd2_idx)\n",
    "    train_idx = total_idx.difference(test_idx)\n",
    "\n",
    "    # return 실제 ckd, 실제 ckd 갯수와 동일한 갯수의 subject, control_idx - ckd_idx\n",
    "    return unbalanced_data.loc[train_idx], unbalanced_data.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Oversampling\n",
    "def oversampling(unbalanced_dataframe, seed):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote = SMOTE(random_state=seed)\n",
    "    temp = unbalanced_dataframe.drop(['RID'], axis=1)\n",
    "    X_train, y_train = smote.fit_resample(temp, temp['onset_tight'])\n",
    "\n",
    "    # X_train에는 RID, onset_3 없음.\n",
    "    return X_train.drop(['onset_tight'], axis=1), y_train\n",
    "\n",
    "### Undersampling\n",
    "# test에 나머지 control sample 추가해서 idx 만 반환\n",
    "def _under_sampling_idx(unbalanced_data, seed):\n",
    "    # train에서 ckd, control index 확인\n",
    "    ckd2_idx = unbalanced_data[unbalanced_data['onset_tight'] == 2].index        # eGFR > 60 & urineprotein > 1+\n",
    "    ckd1_idx = unbalanced_data[unbalanced_data['onset_tight'] == 1].index        # 실제 ckd\n",
    "    control_idx = unbalanced_data[unbalanced_data['onset_tight'] == 0].index    # 실제 control\n",
    "    # print(control_idx, ckd_idx)\n",
    "\n",
    "    # ckd 갯수와 동일하게 control idx sampling\n",
    "    \"\"\"\n",
    "    Control CKD 비율 조정\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed=seed) \n",
    "    sampled_control_idx = pd.Index(rng.choice(control_idx, size=len(ckd1_idx), replace=False)) # ckd 갯수와 동일하게 sampling\n",
    "    not_sampled_control_idx = control_idx.difference(sampled_control_idx)\n",
    "    \n",
    "    sampled_ckd2_idx = pd.Index(rng.choice(ckd2_idx, size=len(ckd1_idx), replace=False)) # ckd 갯수와 동일하게 sampling\n",
    "    not_sampled_control_idx = ckd2_idx.difference(sampled_ckd2_idx)\n",
    "\n",
    "    # 잘 sampling 되었는지 확인\n",
    "    assert set(sampled_control_idx).issubset(set(control_idx))\n",
    "    assert set(sampled_ckd2_idx).issubset(set(ckd2_idx))\n",
    "    # print(len(sampled_control_idx))\n",
    "\n",
    "    balanced_idx = sampled_control_idx.append(ckd1_idx)\n",
    "    balanced_idx = balanced_idx.append(sampled_ckd2_idx)\n",
    "\n",
    "    # return 실제 ckd, 실제 ckd 갯수와 동일한 갯수의 subject, control_idx - ckd_idx\n",
    "    return ckd1_idx, sampled_control_idx, not_sampled_control_idx, balanced_idx\n",
    "\n",
    "def undersampling(unbalanced_data, seed):\n",
    "    a, b, c, d = _under_sampling_idx(unbalanced_data, seed)\n",
    "    under_sampled_data = unbalanced_data.loc[d]\n",
    "    X_undersampled = under_sampled_data.drop(['RID', 'onset_tight'], axis=1)\n",
    "    y_undersampled = under_sampled_data['onset_tight']\n",
    "    return X_undersampled, y_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(y_test, final_prediction, final_probabilities):\n",
    "    cm = confusion_matrix(list(y_test), list(final_prediction))\n",
    "    print(cm)\n",
    "    tn, fn, tp, fp  = cm[0][0], cm[1][0], cm[1][1], cm[0][1]\n",
    "    recall = tp / (fn + tp)\n",
    "    precision = tp / (fp + tp)\n",
    "    acc = (tp + tn) / (tn + fn + tp + fp)\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, final_probabilities, multi_class=\"ovr\")\n",
    "    \n",
    "    print(\"Recall \\t Precision \\t Acc \\t AUC\")\n",
    "    print(f\"{np.round(recall, 4)} {np.round(precision, 4)} {np.round(acc, 4)} {np.round(auc, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fitting(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def model_eval(fitted_model, X_test, y_test):\n",
    "    y_pred = fitted_model.predict(X_test)\n",
    "    y_pred_prob = fitted_model.predict_proba(X_test)  # AUC 계산을 위해 확률값 사용    \n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')  # 다중 클래스이므로 'macro' 사용\n",
    "    recall = recall_score(y_test, y_pred, average='macro')        # 다중 클래스이므로 'macro' 사용\n",
    "    # auc = roc_auc_score(y_bin[test_index], y_pred_prob, multi_class=\"ovr\")  # 다중 클래스 AUC\n",
    "\n",
    "    print(\"Recall \\t Precision \\t Acc\")\n",
    "    print(f\"{np.round(recall, 4)} {np.round(precision, 4)} {np.round(accuracy, 4)}\")\n",
    "    \n",
    "# def model_eval(fitted_model, X_test, y_test):\n",
    "#     model_prediction = fitted_model.predict(X_test)\n",
    "#     model_probabilities = fitted_model.predict_proba(X_test)[:, 1]\n",
    "#     model_score = fitted_model.score(X_test, y_test)\n",
    "#     print(f\"Score with simple {fitted_model} model\")\n",
    "#     print(0.5, np.round(model_score, 4))     # accuracy\n",
    "\n",
    "#     get_results(y_test, model_prediction, model_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Food sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum = pd.read_csv(\"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data/0922_3way_data/3way_basic_food_sum.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset  onset_tight\n",
      "0      0              55569\n",
      "1      0               1137\n",
      "       1                525\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tight3_foodsum[['onset', 'onset_tight']].value_counts())\n",
    "\n",
    "# Urine protein 1+ & eGFR >= 60 인 환자 (1137명) 2로 labeling\n",
    "tight3_foodsum.loc[(tight3_foodsum['onset'] == 1) & (tight3_foodsum['onset_tight'] == 0), 'onset_tight'] = 2\n",
    "tight3_foodsum['onset_tight'].value_counts()\n",
    "\n",
    "tight3_foodsum.drop(['onset'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.54761905 0.58333333 0.58333333 0.60714286 0.56746032]\n",
      "각 폴드의 Precision: [0.53706246 0.57842919 0.58888426 0.59193644 0.56529791]\n",
      "각 폴드의 Accuracy: [0.54761905 0.58333333 0.58333333 0.60714286 0.56746032]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5778, 0.5723, 0.5778\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5841 0.5855 0.5841\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.57936508 0.57539683 0.5952381  0.56746032 0.57142857]\n",
      "각 폴드의 Precision: [0.57303355 0.55951152 0.58259932 0.54673282 0.55859686]\n",
      "각 폴드의 Accuracy: [0.57936508 0.57539683 0.5952381  0.56746032 0.57142857]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5778, 0.5641, 0.5778\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5619 0.5418 0.5619\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.5515873  0.59920635 0.57936508 0.59920635 0.57539683]\n",
      "각 폴드의 Precision: [0.54647596 0.59760621 0.57758128 0.58103599 0.56666667]\n",
      "각 폴드의 Accuracy: [0.5515873  0.59920635 0.57936508 0.59920635 0.57539683]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.581, 0.5739, 0.581\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5746 0.5699 0.5746\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.53571429 0.51190476 0.6031746  0.56349206 0.56349206]\n",
      "각 폴드의 Precision: [0.53399196 0.49925517 0.60361608 0.54907519 0.54336839]\n",
      "각 폴드의 Accuracy: [0.53571429 0.51190476 0.6031746  0.56349206 0.56349206]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5556, 0.5459, 0.5556\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5238 0.5218 0.5238\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.58333333 0.59126984 0.57539683 0.52777778 0.6031746 ]\n",
      "각 폴드의 Precision: [0.56904912 0.58438302 0.56953699 0.51688224 0.5886798 ]\n",
      "각 폴드의 Accuracy: [0.58333333 0.59126984 0.57539683 0.52777778 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5762, 0.5657, 0.5762\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5746 0.5635 0.5746\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.60714286 0.55952381 0.57539683 0.52777778 0.54761905]\n",
      "각 폴드의 Precision: [0.59161616 0.54190482 0.56592181 0.50159273 0.51021103]\n",
      "각 폴드의 Accuracy: [0.60714286 0.55952381 0.57539683 0.52777778 0.54761905]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5635, 0.5422, 0.5635\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5492 0.5175 0.5492\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.5515873  0.59126984 0.57142857 0.50793651 0.59126984]\n",
      "각 폴드의 Precision: [0.53126458 0.57829457 0.56175806 0.49344159 0.58249158]\n",
      "각 폴드의 Accuracy: [0.5515873  0.59126984 0.57142857 0.50793651 0.59126984]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5627, 0.5495, 0.5627\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5651 0.5585 0.5651\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.49603175 0.56349206 0.56349206 0.53174603 0.55555556]\n",
      "각 폴드의 Precision: [0.48106629 0.53846862 0.55089928 0.53113556 0.55349881]\n",
      "각 폴드의 Accuracy: [0.49603175 0.56349206 0.56349206 0.53174603 0.55555556]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5421, 0.531, 0.5421\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5492 0.5514 0.5492\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.5515873  0.56746032 0.64285714 0.56746032 0.58730159]\n",
      "각 폴드의 Precision: [0.54328272 0.55893787 0.63260836 0.56209453 0.58204607]\n",
      "각 폴드의 Accuracy: [0.5515873  0.56746032 0.64285714 0.56746032 0.58730159]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5758, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5714 0.5721 0.5714\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.59126984 0.5515873  0.56349206 0.58730159 0.56746032]\n",
      "각 폴드의 Precision: [0.57085109 0.52929307 0.54317685 0.57524085 0.5411425 ]\n",
      "각 폴드의 Accuracy: [0.59126984 0.5515873  0.56349206 0.58730159 0.56746032]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5722, 0.5519, 0.5722\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5683 0.5521 0.5683\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.54365079 0.57142857 0.62698413 0.57936508 0.57936508]\n",
      "각 폴드의 Precision: [0.53569508 0.55701121 0.61389641 0.57154255 0.5755814 ]\n",
      "각 폴드의 Accuracy: [0.54365079 0.57142857 0.62698413 0.57936508 0.57936508]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5802, 0.5707, 0.5802\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5651 0.5623 0.5651\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.52380952 0.57539683 0.50396825 0.52380952 0.57142857]\n",
      "각 폴드의 Precision: [0.51611268 0.55827406 0.48304919 0.50631045 0.55861432]\n",
      "각 폴드의 Accuracy: [0.52380952 0.57539683 0.50396825 0.52380952 0.57142857]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5397, 0.5245, 0.5397\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5333 0.5179 0.5333\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.58730159 0.57539683 0.58333333 0.54761905 0.61507937]\n",
      "각 폴드의 Precision: [0.5734799  0.5634713  0.57682407 0.55447505 0.61947553]\n",
      "각 폴드의 Accuracy: [0.58730159 0.57539683 0.58333333 0.54761905 0.61507937]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5817, 0.5775, 0.5817\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5429 0.5364 0.5429\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.56746032 0.60714286 0.57539683 0.56746032 0.62301587]\n",
      "각 폴드의 Precision: [0.53975113 0.59013081 0.56320036 0.55670013 0.61300561]\n",
      "각 폴드의 Accuracy: [0.56746032 0.60714286 0.57539683 0.56746032 0.62301587]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5881, 0.5726, 0.5881\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.546 0.5293 0.546\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.59126984 0.58333333 0.58333333 0.55952381 0.63095238]\n",
      "각 폴드의 Precision: [0.57117595 0.5712579  0.57655784 0.55589509 0.63052326]\n",
      "각 폴드의 Accuracy: [0.59126984 0.58333333 0.58333333 0.55952381 0.63095238]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5897, 0.5811, 0.5897\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5397 0.533 0.5397\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.54761905 0.54365079 0.56746032 0.52777778 0.54761905]\n",
      "각 폴드의 Precision: [0.5459197  0.51616526 0.56750162 0.54735252 0.55887659]\n",
      "각 폴드의 Accuracy: [0.54761905 0.54365079 0.56746032 0.52777778 0.54761905]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5468, 0.5472, 0.5468\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5079 0.5203 0.5079\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.53174603 0.61904762 0.61904762 0.54365079 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52473151 0.61670793 0.61278999 0.53243616 0.59647153]\n",
      "각 폴드의 Accuracy: [0.53174603 0.61904762 0.61904762 0.54365079 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5766, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5556 0.562 0.5556\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.54761905 0.60714286 0.60714286 0.53571429 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52978754 0.6012025  0.5952344  0.51257355 0.59736667]\n",
      "각 폴드의 Accuracy: [0.54761905 0.60714286 0.60714286 0.53571429 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5802, 0.5672, 0.5802\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5937 0.5823 0.5937\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.55952381 0.61111111 0.61904762 0.53571429 0.62301587]\n",
      "각 폴드의 Precision: [0.54741066 0.60660194 0.60883236 0.52690694 0.61914539]\n",
      "각 폴드의 Accuracy: [0.55952381 0.61111111 0.61904762 0.53571429 0.62301587]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5897, 0.5818, 0.5897\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5873 0.5863 0.5873\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.52777778 0.55555556 0.55952381 0.51587302 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52269485 0.549209   0.55373058 0.50468681 0.64703602]\n",
      "각 폴드의 Accuracy: [0.52777778 0.55555556 0.55952381 0.51587302 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5524, 0.5555, 0.5524\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5333 0.528 0.5333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_split = 5\n",
    "\n",
    "# Train/test divide => Always Same\n",
    "tight3_foodsum_train, tight3_foodsum_test = divide_testset(tight3_foodsum, 0.2)        # train-test ratio\n",
    "y_tight3_test = tight3_foodsum_test['onset_tight']\n",
    "X_tight3_test = tight3_foodsum_test.drop(['RID', 'onset_tight'], axis=1)\n",
    "\n",
    "seed_results = {}\n",
    "for seed in [42, 59, 63, 79, 101]:    \n",
    "    datas = {\"original\":(), \"undersampling\":(), \"oversampling\":(), \"test\":(X_tight3_test, y_tight3_test)}\n",
    "\n",
    "    datas['undersampling'] = undersampling(tight3_foodsum_train, seed=seed)\n",
    "    datas['oversampling'] = oversampling(tight3_foodsum_train, seed=seed)\n",
    "    y_tight3_foodsum_train = tight3_foodsum_train['onset_tight']\n",
    "    X_tight3_foodsum_train = tight3_foodsum_train.drop(['RID', 'onset_tight'], axis=1)\n",
    "    datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "    \n",
    "    for data in ['undersampling']:\n",
    "        print(f\"For {data} dataset!!!\")\n",
    "        X_test, y_test = datas['test']\n",
    "        X_train, y_train = datas[data]\n",
    "        print(y_train.value_counts())\n",
    "        \n",
    "        wei_train_scaler = StandardScaler()\n",
    "        X_train = wei_train_scaler.fit_transform(X_train)\n",
    "        X_test = wei_train_scaler.transform(X_test)\n",
    "        print(f\"{data} dataset loaded and scaled\")\n",
    "        \n",
    "        scoring = {\n",
    "        'recall': 'recall_macro',      # recall for each class, then averaged\n",
    "        'precision': 'precision_macro',# precision for each class, then averaged\n",
    "        'accuracy': 'accuracy',        # accuracy\n",
    "        # 'auc': make_scorer(roc_auc_score, multi_class='ovr')  # AUC 계산 (이진 분류시)\n",
    "        }\n",
    "        \n",
    "        models = (\n",
    "            SVC(kernel='linear', random_state=seed, probability=True),\n",
    "            RandomForestClassifier(random_state=seed, max_depth=3),\n",
    "            LogisticRegression(max_iter=1000, random_state=seed),\n",
    "            GradientBoostingClassifier(random_state=seed, max_depth=1, learning_rate=0.01),\n",
    "        )\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            print(\"=\" * 30)\n",
    "            print(f\"Cross Validation for {model} in fold {n_split}, with seed {seed}\")\n",
    "            # # Stratified K-Fold 교차 검증 (K=5)\n",
    "            skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "            results = cross_validate(model, X_train, y_train, cv=skf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            # 결과 출력\n",
    "            print(\"각 폴드의 Recall:\", results['test_recall'])\n",
    "            print(\"각 폴드의 Precision:\", results['test_precision'])\n",
    "            print(\"각 폴드의 Accuracy:\", results['test_accuracy'])\n",
    "            # print(\"각 폴드의 AUC:\", results['test_auc'])\n",
    "\n",
    "            # 평균값 계산\n",
    "            print(\"평균 Recall, Precision, Accuracy, AUC:\")\n",
    "            print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}\")\n",
    "            # print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}, {round(np.mean(results['test_auc']), 4)}\")\n",
    "            \n",
    "            if seed not in seed_results.keys():\n",
    "                seed_results[seed] = [(round(np.mean(results['test_recall']), 4),\n",
    "                                    round(np.mean(results['test_precision']),4),\n",
    "                                    round(np.mean(results['test_accuracy']), 4))]\n",
    "            else:\n",
    "                seed_results[seed].append((round(np.mean(results['test_recall']), 4),\n",
    "                                    round(np.mean(results['test_precision']),4),\n",
    "                                    round(np.mean(results['test_accuracy']), 4)))\n",
    "            \n",
    "            print()\n",
    "            print(\"-- Final Model Train & Eval --\")\n",
    "            final_model = models[i]\n",
    "            final_model = model_fitting(final_model, X_train, y_train)\n",
    "            model_eval(final_model, X_test, y_test)\n",
    "            print()\n",
    "\n",
    "print(seed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{42: [(0.5778, 0.5723, 0.5778), (0.5778, 0.5641, 0.5778), (0.581, 0.5739, 0.581), (0.5556, 0.5459, 0.5556)], 59: [(0.5762, 0.5657, 0.5762), (0.5635, 0.5422, 0.5635), (0.5627, 0.5495, 0.5627), (0.5421, 0.531, 0.5421)], 63: [(0.5833, 0.5758, 0.5833), (0.5722, 0.5519, 0.5722), (0.5802, 0.5707, 0.5802), (0.5397, 0.5245, 0.5397)], 79: [(0.5817, 0.5775, 0.5817), (0.5881, 0.5726, 0.5881), (0.5897, 0.5811, 0.5897), (0.5468, 0.5472, 0.5468)], 101: [(0.5833, 0.5766, 0.5833), (0.5802, 0.5672, 0.5802), (0.5897, 0.5818, 0.5897), (0.5524, 0.5555, 0.5524)]}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Food mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum = pd.read_csv(\"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data/0922_3way_data/3way_basic_food_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset  onset_tight\n",
      "0      0              55569\n",
      "1      0               1137\n",
      "       1                525\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tight3_foodsum[['onset', 'onset_tight']].value_counts())\n",
    "\n",
    "# Urine protein 1+ & eGFR >= 60 인 환자 (1137명) 2로 labeling\n",
    "tight3_foodsum.loc[(tight3_foodsum['onset'] == 1) & (tight3_foodsum['onset_tight'] == 0), 'onset_tight'] = 2\n",
    "tight3_foodsum['onset_tight'].value_counts()\n",
    "\n",
    "tight3_foodsum.drop(['onset'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.54761905 0.58333333 0.58333333 0.60714286 0.56746032]\n",
      "각 폴드의 Precision: [0.53706246 0.57842919 0.58888426 0.59193644 0.56529791]\n",
      "각 폴드의 Accuracy: [0.54761905 0.58333333 0.58333333 0.60714286 0.56746032]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5778, 0.5723, 0.5778\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5841 0.5855 0.5841\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.57936508 0.57539683 0.5952381  0.56746032 0.57142857]\n",
      "각 폴드의 Precision: [0.57303355 0.55951152 0.58259932 0.54673282 0.55859686]\n",
      "각 폴드의 Accuracy: [0.57936508 0.57539683 0.5952381  0.56746032 0.57142857]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5778, 0.5641, 0.5778\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5619 0.5418 0.5619\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.5515873  0.59920635 0.57936508 0.59920635 0.57539683]\n",
      "각 폴드의 Precision: [0.54647596 0.59760621 0.57758128 0.58103599 0.56666667]\n",
      "각 폴드의 Accuracy: [0.5515873  0.59920635 0.57936508 0.59920635 0.57539683]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.581, 0.5739, 0.581\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5746 0.5699 0.5746\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.53571429 0.51190476 0.6031746  0.56349206 0.56349206]\n",
      "각 폴드의 Precision: [0.53399196 0.49925517 0.60361608 0.54907519 0.54336839]\n",
      "각 폴드의 Accuracy: [0.53571429 0.51190476 0.6031746  0.56349206 0.56349206]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5556, 0.5459, 0.5556\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5238 0.5218 0.5238\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.58333333 0.59126984 0.57539683 0.52777778 0.6031746 ]\n",
      "각 폴드의 Precision: [0.56904912 0.58438302 0.56953699 0.51688224 0.5886798 ]\n",
      "각 폴드의 Accuracy: [0.58333333 0.59126984 0.57539683 0.52777778 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5762, 0.5657, 0.5762\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5746 0.5635 0.5746\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.60714286 0.55952381 0.57539683 0.52777778 0.54761905]\n",
      "각 폴드의 Precision: [0.59161616 0.54190482 0.56592181 0.50159273 0.51021103]\n",
      "각 폴드의 Accuracy: [0.60714286 0.55952381 0.57539683 0.52777778 0.54761905]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5635, 0.5422, 0.5635\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5492 0.5175 0.5492\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.5515873  0.59126984 0.57142857 0.50793651 0.59126984]\n",
      "각 폴드의 Precision: [0.53126458 0.57829457 0.56175806 0.49344159 0.58249158]\n",
      "각 폴드의 Accuracy: [0.5515873  0.59126984 0.57142857 0.50793651 0.59126984]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5627, 0.5495, 0.5627\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5651 0.5585 0.5651\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.49603175 0.56349206 0.56349206 0.53174603 0.55555556]\n",
      "각 폴드의 Precision: [0.48106629 0.53846862 0.55089928 0.53113556 0.55349881]\n",
      "각 폴드의 Accuracy: [0.49603175 0.56349206 0.56349206 0.53174603 0.55555556]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5421, 0.531, 0.5421\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5492 0.5514 0.5492\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.5515873  0.56746032 0.64285714 0.56746032 0.58730159]\n",
      "각 폴드의 Precision: [0.54328272 0.55893787 0.63260836 0.56209453 0.58204607]\n",
      "각 폴드의 Accuracy: [0.5515873  0.56746032 0.64285714 0.56746032 0.58730159]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5758, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5714 0.5721 0.5714\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.59126984 0.5515873  0.56349206 0.58730159 0.56746032]\n",
      "각 폴드의 Precision: [0.57085109 0.52929307 0.54317685 0.57524085 0.5411425 ]\n",
      "각 폴드의 Accuracy: [0.59126984 0.5515873  0.56349206 0.58730159 0.56746032]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5722, 0.5519, 0.5722\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5683 0.5521 0.5683\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.54365079 0.57142857 0.62698413 0.57936508 0.57936508]\n",
      "각 폴드의 Precision: [0.53569508 0.55701121 0.61389641 0.57154255 0.5755814 ]\n",
      "각 폴드의 Accuracy: [0.54365079 0.57142857 0.62698413 0.57936508 0.57936508]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5802, 0.5707, 0.5802\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5651 0.5623 0.5651\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.52380952 0.57539683 0.50396825 0.52380952 0.57142857]\n",
      "각 폴드의 Precision: [0.51611268 0.55827406 0.48304919 0.50631045 0.55861432]\n",
      "각 폴드의 Accuracy: [0.52380952 0.57539683 0.50396825 0.52380952 0.57142857]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5397, 0.5245, 0.5397\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5333 0.5179 0.5333\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.58730159 0.57539683 0.58333333 0.54761905 0.61507937]\n",
      "각 폴드의 Precision: [0.5734799  0.5634713  0.57682407 0.55447505 0.61947553]\n",
      "각 폴드의 Accuracy: [0.58730159 0.57539683 0.58333333 0.54761905 0.61507937]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5817, 0.5775, 0.5817\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5429 0.5364 0.5429\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.56746032 0.60714286 0.57539683 0.56746032 0.62301587]\n",
      "각 폴드의 Precision: [0.53975113 0.59013081 0.56320036 0.55670013 0.61300561]\n",
      "각 폴드의 Accuracy: [0.56746032 0.60714286 0.57539683 0.56746032 0.62301587]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5881, 0.5726, 0.5881\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.546 0.5293 0.546\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.59126984 0.58333333 0.58333333 0.55952381 0.63095238]\n",
      "각 폴드의 Precision: [0.57117595 0.5712579  0.57655784 0.55589509 0.63052326]\n",
      "각 폴드의 Accuracy: [0.59126984 0.58333333 0.58333333 0.55952381 0.63095238]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5897, 0.5811, 0.5897\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5397 0.533 0.5397\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.54761905 0.54365079 0.56746032 0.52777778 0.54761905]\n",
      "각 폴드의 Precision: [0.5459197  0.51616526 0.56750162 0.54735252 0.55887659]\n",
      "각 폴드의 Accuracy: [0.54761905 0.54365079 0.56746032 0.52777778 0.54761905]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5468, 0.5472, 0.5468\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5079 0.5203 0.5079\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.53174603 0.61904762 0.61904762 0.54365079 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52473151 0.61670793 0.61278999 0.53243616 0.59647153]\n",
      "각 폴드의 Accuracy: [0.53174603 0.61904762 0.61904762 0.54365079 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5766, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5556 0.562 0.5556\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.54761905 0.60714286 0.60714286 0.53571429 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52978754 0.6012025  0.5952344  0.51257355 0.59736667]\n",
      "각 폴드의 Accuracy: [0.54761905 0.60714286 0.60714286 0.53571429 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5802, 0.5672, 0.5802\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5937 0.5823 0.5937\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.55952381 0.61111111 0.61904762 0.53571429 0.62301587]\n",
      "각 폴드의 Precision: [0.54741066 0.60660194 0.60883236 0.52690694 0.61914539]\n",
      "각 폴드의 Accuracy: [0.55952381 0.61111111 0.61904762 0.53571429 0.62301587]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5897, 0.5818, 0.5897\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5873 0.5863 0.5873\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.52777778 0.55555556 0.55952381 0.51587302 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52269485 0.549209   0.55373058 0.50468681 0.64703602]\n",
      "각 폴드의 Accuracy: [0.52777778 0.55555556 0.55952381 0.51587302 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5524, 0.5555, 0.5524\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5333 0.528 0.5333\n",
      "\n",
      "{42: [(0.5778, 0.5723, 0.5778), (0.5778, 0.5641, 0.5778), (0.581, 0.5739, 0.581), (0.5556, 0.5459, 0.5556)], 59: [(0.5762, 0.5657, 0.5762), (0.5635, 0.5422, 0.5635), (0.5627, 0.5495, 0.5627), (0.5421, 0.531, 0.5421)], 63: [(0.5833, 0.5758, 0.5833), (0.5722, 0.5519, 0.5722), (0.5802, 0.5707, 0.5802), (0.5397, 0.5245, 0.5397)], 79: [(0.5817, 0.5775, 0.5817), (0.5881, 0.5726, 0.5881), (0.5897, 0.5811, 0.5897), (0.5468, 0.5472, 0.5468)], 101: [(0.5833, 0.5766, 0.5833), (0.5802, 0.5672, 0.5802), (0.5897, 0.5818, 0.5897), (0.5524, 0.5555, 0.5524)]}\n"
     ]
    }
   ],
   "source": [
    "n_split = 5\n",
    "\n",
    "# Train/test divide => Always Same\n",
    "tight3_foodsum_train, tight3_foodsum_test = divide_testset(tight3_foodsum, 0.2)        # train-test ratio\n",
    "y_tight3_test = tight3_foodsum_test['onset_tight']\n",
    "X_tight3_test = tight3_foodsum_test.drop(['RID', 'onset_tight'], axis=1)\n",
    "\n",
    "seed_results = {}\n",
    "for seed in [42, 59, 63, 79, 101]:    \n",
    "    datas = {\"original\":(), \"undersampling\":(), \"oversampling\":(), \"test\":(X_tight3_test, y_tight3_test)}\n",
    "\n",
    "    datas['undersampling'] = undersampling(tight3_foodsum_train, seed=seed)\n",
    "    datas['oversampling'] = oversampling(tight3_foodsum_train, seed=seed)\n",
    "    y_tight3_foodsum_train = tight3_foodsum_train['onset_tight']\n",
    "    X_tight3_foodsum_train = tight3_foodsum_train.drop(['RID', 'onset_tight'], axis=1)\n",
    "    datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "    \n",
    "    for data in ['undersampling']:\n",
    "        print(f\"For {data} dataset!!!\")\n",
    "        X_test, y_test = datas['test']\n",
    "        X_train, y_train = datas[data]\n",
    "        print(y_train.value_counts())\n",
    "        \n",
    "        wei_train_scaler = StandardScaler()\n",
    "        X_train = wei_train_scaler.fit_transform(X_train)\n",
    "        X_test = wei_train_scaler.transform(X_test)\n",
    "        print(f\"{data} dataset loaded and scaled\")\n",
    "        \n",
    "        scoring = {\n",
    "        'recall': 'recall_macro',      # recall for each class, then averaged\n",
    "        'precision': 'precision_macro',# precision for each class, then averaged\n",
    "        'accuracy': 'accuracy',        # accuracy\n",
    "        # 'auc': make_scorer(roc_auc_score, multi_class='ovr')  # AUC 계산 (이진 분류시)\n",
    "        }\n",
    "        \n",
    "        models = (\n",
    "            SVC(kernel='linear', random_state=seed, probability=True),\n",
    "            RandomForestClassifier(random_state=seed, max_depth=3),\n",
    "            LogisticRegression(max_iter=1000, random_state=seed),\n",
    "            GradientBoostingClassifier(random_state=seed, max_depth=1, learning_rate=0.01),\n",
    "        )\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            print(\"=\" * 30)\n",
    "            print(f\"Cross Validation for {model} in fold {n_split}, with seed {seed}\")\n",
    "            # # Stratified K-Fold 교차 검증 (K=5)\n",
    "            skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "            results = cross_validate(model, X_train, y_train, cv=skf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            # 결과 출력\n",
    "            print(\"각 폴드의 Recall:\", results['test_recall'])\n",
    "            print(\"각 폴드의 Precision:\", results['test_precision'])\n",
    "            print(\"각 폴드의 Accuracy:\", results['test_accuracy'])\n",
    "            # print(\"각 폴드의 AUC:\", results['test_auc'])\n",
    "\n",
    "            # 평균값 계산\n",
    "            print(\"평균 Recall, Precision, Accuracy, AUC:\")\n",
    "            print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}\")\n",
    "            # print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}, {round(np.mean(results['test_auc']), 4)}\")\n",
    "            \n",
    "            if seed not in seed_results.keys():\n",
    "                seed_results[seed] = [(round(np.mean(results['test_recall']), 4),\n",
    "                                    round(np.mean(results['test_precision']),4),\n",
    "                                    round(np.mean(results['test_accuracy']), 4))]\n",
    "            else:\n",
    "                seed_results[seed].append((round(np.mean(results['test_recall']), 4),\n",
    "                                    round(np.mean(results['test_precision']),4),\n",
    "                                    round(np.mean(results['test_accuracy']), 4)))\n",
    "            \n",
    "            print()\n",
    "            print(\"-- Final Model Train & Eval --\")\n",
    "            final_model = models[i]\n",
    "            final_model = model_fitting(final_model, X_train, y_train)\n",
    "            model_eval(final_model, X_test, y_test)\n",
    "            print()\n",
    "\n",
    "print(seed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Food adjsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum = pd.read_csv(\"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data/0922_3way_data/3way_basic_food_adjusted_sum.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset  onset_tight\n",
      "0      0              55569\n",
      "1      0               1137\n",
      "       1                525\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tight3_foodsum[['onset', 'onset_tight']].value_counts())\n",
    "\n",
    "# Urine protein 1+ & eGFR >= 60 인 환자 (1137명) 2로 labeling\n",
    "tight3_foodsum.loc[(tight3_foodsum['onset'] == 1) & (tight3_foodsum['onset_tight'] == 0), 'onset_tight'] = 2\n",
    "tight3_foodsum['onset_tight'].value_counts()\n",
    "\n",
    "tight3_foodsum.drop(['onset'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.54761905 0.58333333 0.58730159 0.60714286 0.56746032]\n",
      "각 폴드의 Precision: [0.53706246 0.57842919 0.59262646 0.59193644 0.56529791]\n",
      "각 폴드의 Accuracy: [0.54761905 0.58333333 0.58730159 0.60714286 0.56746032]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5786, 0.5731, 0.5786\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5873 0.5881 0.5873\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.56349206 0.59920635 0.58333333 0.59126984 0.58333333]\n",
      "각 폴드의 Precision: [0.55572836 0.59387374 0.5710728  0.57607691 0.57031626]\n",
      "각 폴드의 Accuracy: [0.56349206 0.59920635 0.58333333 0.59126984 0.58333333]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5841, 0.5734, 0.5841\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5778 0.5639 0.5778\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.5515873  0.59920635 0.57936508 0.59920635 0.57539683]\n",
      "각 폴드의 Precision: [0.54647596 0.59760621 0.57758128 0.58103599 0.56666667]\n",
      "각 폴드의 Accuracy: [0.5515873  0.59920635 0.57936508 0.59920635 0.57539683]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.581, 0.5739, 0.581\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5746 0.5699 0.5746\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.53571429 0.51190476 0.6031746  0.56349206 0.56349206]\n",
      "각 폴드의 Precision: [0.53399196 0.49925517 0.60361608 0.54907519 0.54336839]\n",
      "각 폴드의 Accuracy: [0.53571429 0.51190476 0.6031746  0.56349206 0.56349206]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5556, 0.5459, 0.5556\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5238 0.5218 0.5238\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.58333333 0.59126984 0.57539683 0.52777778 0.6031746 ]\n",
      "각 폴드의 Precision: [0.56904912 0.58438302 0.56953699 0.51688224 0.5886798 ]\n",
      "각 폴드의 Accuracy: [0.58333333 0.59126984 0.57539683 0.52777778 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5762, 0.5657, 0.5762\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5746 0.563 0.5746\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.57142857 0.57142857 0.57142857 0.5        0.52380952]\n",
      "각 폴드의 Precision: [0.556232   0.55587801 0.556505   0.47561591 0.48962639]\n",
      "각 폴드의 Accuracy: [0.57142857 0.57142857 0.57142857 0.5        0.52380952]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5476, 0.5268, 0.5476\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.546 0.516 0.546\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.5515873  0.59126984 0.57142857 0.50793651 0.59126984]\n",
      "각 폴드의 Precision: [0.53126458 0.57829457 0.56175806 0.49344159 0.58249158]\n",
      "각 폴드의 Accuracy: [0.5515873  0.59126984 0.57142857 0.50793651 0.59126984]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5627, 0.5495, 0.5627\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5651 0.5585 0.5651\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.49603175 0.56349206 0.56349206 0.53174603 0.55555556]\n",
      "각 폴드의 Precision: [0.48106629 0.53846862 0.55089928 0.53113556 0.55349881]\n",
      "각 폴드의 Accuracy: [0.49603175 0.56349206 0.56349206 0.53174603 0.55555556]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5421, 0.531, 0.5421\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5492 0.5514 0.5492\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.5515873  0.57142857 0.64285714 0.56349206 0.58730159]\n",
      "각 폴드의 Precision: [0.54328272 0.56286654 0.63330314 0.55752916 0.58204607]\n",
      "각 폴드의 Accuracy: [0.5515873  0.57142857 0.64285714 0.56349206 0.58730159]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5758, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5714 0.5723 0.5714\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.58333333 0.53968254 0.55555556 0.56349206 0.55952381]\n",
      "각 폴드의 Precision: [0.56672953 0.52024533 0.5379989  0.54747135 0.53331452]\n",
      "각 폴드의 Accuracy: [0.58333333 0.53968254 0.55555556 0.56349206 0.55952381]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5603, 0.5412, 0.5603\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5524 0.5332 0.5524\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.54761905 0.57142857 0.62698413 0.57936508 0.57936508]\n",
      "각 폴드의 Precision: [0.53819452 0.55701121 0.61389641 0.57154255 0.5755814 ]\n",
      "각 폴드의 Accuracy: [0.54761905 0.57142857 0.62698413 0.57936508 0.57936508]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.581, 0.5712, 0.581\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5651 0.5623 0.5651\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.52380952 0.57539683 0.50396825 0.52380952 0.57142857]\n",
      "각 폴드의 Precision: [0.51611268 0.55827406 0.48304919 0.50631045 0.55861432]\n",
      "각 폴드의 Accuracy: [0.52380952 0.57539683 0.50396825 0.52380952 0.57142857]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5397, 0.5245, 0.5397\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5333 0.5179 0.5333\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.58730159 0.57142857 0.58730159 0.5515873  0.61904762]\n",
      "각 폴드의 Precision: [0.5734799  0.5584453  0.58113878 0.5585093  0.62374612]\n",
      "각 폴드의 Accuracy: [0.58730159 0.57142857 0.58730159 0.5515873  0.61904762]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5791, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.546 0.5394 0.546\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.57936508 0.57936508 0.57142857 0.56349206 0.61507937]\n",
      "각 폴드의 Precision: [0.55772351 0.55782828 0.56209194 0.55071352 0.60513664]\n",
      "각 폴드의 Accuracy: [0.57936508 0.57936508 0.57142857 0.56349206 0.61507937]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5817, 0.5667, 0.5817\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5206 0.5037 0.5206\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.59126984 0.58333333 0.58333333 0.55952381 0.63095238]\n",
      "각 폴드의 Precision: [0.57117595 0.5712579  0.57655784 0.55589509 0.63052326]\n",
      "각 폴드의 Accuracy: [0.59126984 0.58333333 0.58333333 0.55952381 0.63095238]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5897, 0.5811, 0.5897\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5397 0.533 0.5397\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.54761905 0.54365079 0.56746032 0.52777778 0.54761905]\n",
      "각 폴드의 Precision: [0.5459197  0.51616526 0.56750162 0.54735252 0.55887659]\n",
      "각 폴드의 Accuracy: [0.54761905 0.54365079 0.56746032 0.52777778 0.54761905]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5468, 0.5472, 0.5468\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5079 0.5203 0.5079\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.53174603 0.61507937 0.61904762 0.54761905 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52473151 0.61265179 0.61278999 0.537072   0.59647153]\n",
      "각 폴드의 Accuracy: [0.53174603 0.61507937 0.61904762 0.54761905 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5767, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5587 0.5652 0.5587\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.57539683 0.55555556 0.60714286 0.54365079 0.56746032]\n",
      "각 폴드의 Precision: [0.56668044 0.54452402 0.59427609 0.52225768 0.55557434]\n",
      "각 폴드의 Accuracy: [0.57539683 0.55555556 0.60714286 0.54365079 0.56746032]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5698, 0.5567, 0.5698\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5619 0.5489 0.5619\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.55952381 0.61111111 0.61904762 0.53571429 0.62301587]\n",
      "각 폴드의 Precision: [0.54741066 0.60660194 0.60883236 0.52690694 0.61914539]\n",
      "각 폴드의 Accuracy: [0.55952381 0.61111111 0.61904762 0.53571429 0.62301587]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5897, 0.5818, 0.5897\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5873 0.5863 0.5873\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.52777778 0.56349206 0.55952381 0.51587302 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52269485 0.55818007 0.55373058 0.50468681 0.64703602]\n",
      "각 폴드의 Accuracy: [0.52777778 0.56349206 0.55952381 0.51587302 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.554, 0.5573, 0.554\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5333 0.528 0.5333\n",
      "\n",
      "{42: [(0.5786, 0.5731, 0.5786), (0.5841, 0.5734, 0.5841), (0.581, 0.5739, 0.581), (0.5556, 0.5459, 0.5556)], 59: [(0.5762, 0.5657, 0.5762), (0.5476, 0.5268, 0.5476), (0.5627, 0.5495, 0.5627), (0.5421, 0.531, 0.5421)], 63: [(0.5833, 0.5758, 0.5833), (0.5603, 0.5412, 0.5603), (0.581, 0.5712, 0.581), (0.5397, 0.5245, 0.5397)], 79: [(0.5833, 0.5791, 0.5833), (0.5817, 0.5667, 0.5817), (0.5897, 0.5811, 0.5897), (0.5468, 0.5472, 0.5468)], 101: [(0.5833, 0.5767, 0.5833), (0.5698, 0.5567, 0.5698), (0.5897, 0.5818, 0.5897), (0.554, 0.5573, 0.554)]}\n"
     ]
    }
   ],
   "source": [
    "n_split = 5\n",
    "\n",
    "# Train/test divide => Always Same\n",
    "tight3_foodsum_train, tight3_foodsum_test = divide_testset(tight3_foodsum, 0.2)        # train-test ratio\n",
    "y_tight3_test = tight3_foodsum_test['onset_tight']\n",
    "X_tight3_test = tight3_foodsum_test.drop(['RID', 'onset_tight'], axis=1)\n",
    "\n",
    "seed_results = {}\n",
    "for seed in [42, 59, 63, 79, 101]:    \n",
    "    datas = {\"original\":(), \"undersampling\":(), \"oversampling\":(), \"test\":(X_tight3_test, y_tight3_test)}\n",
    "\n",
    "    datas['undersampling'] = undersampling(tight3_foodsum_train, seed=seed)\n",
    "    datas['oversampling'] = oversampling(tight3_foodsum_train, seed=seed)\n",
    "    y_tight3_foodsum_train = tight3_foodsum_train['onset_tight']\n",
    "    X_tight3_foodsum_train = tight3_foodsum_train.drop(['RID', 'onset_tight'], axis=1)\n",
    "    datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "    \n",
    "    for data in ['undersampling']:\n",
    "        print(f\"For {data} dataset!!!\")\n",
    "        X_test, y_test = datas['test']\n",
    "        X_train, y_train = datas[data]\n",
    "        print(y_train.value_counts())\n",
    "        \n",
    "        wei_train_scaler = StandardScaler()\n",
    "        X_train = wei_train_scaler.fit_transform(X_train)\n",
    "        X_test = wei_train_scaler.transform(X_test)\n",
    "        print(f\"{data} dataset loaded and scaled\")\n",
    "        \n",
    "        scoring = {\n",
    "        'recall': 'recall_macro',      # recall for each class, then averaged\n",
    "        'precision': 'precision_macro',# precision for each class, then averaged\n",
    "        'accuracy': 'accuracy',        # accuracy\n",
    "        # 'auc': make_scorer(roc_auc_score, multi_class='ovr')  # AUC 계산 (이진 분류시)\n",
    "        }\n",
    "        \n",
    "        models = (\n",
    "            SVC(kernel='linear', random_state=seed, probability=True),\n",
    "            RandomForestClassifier(random_state=seed, max_depth=3),\n",
    "            LogisticRegression(max_iter=1000, random_state=seed),\n",
    "            GradientBoostingClassifier(random_state=seed, max_depth=1, learning_rate=0.01),\n",
    "        )\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            print(\"=\" * 30)\n",
    "            print(f\"Cross Validation for {model} in fold {n_split}, with seed {seed}\")\n",
    "            # # Stratified K-Fold 교차 검증 (K=5)\n",
    "            skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "            results = cross_validate(model, X_train, y_train, cv=skf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            # 결과 출력\n",
    "            print(\"각 폴드의 Recall:\", results['test_recall'])\n",
    "            print(\"각 폴드의 Precision:\", results['test_precision'])\n",
    "            print(\"각 폴드의 Accuracy:\", results['test_accuracy'])\n",
    "            # print(\"각 폴드의 AUC:\", results['test_auc'])\n",
    "\n",
    "            # 평균값 계산\n",
    "            print(\"평균 Recall, Precision, Accuracy, AUC:\")\n",
    "            print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}\")\n",
    "            # print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}, {round(np.mean(results['test_auc']), 4)}\")\n",
    "            \n",
    "            if seed not in seed_results.keys():\n",
    "                seed_results[seed] = [(round(np.mean(results['test_recall']), 4),\n",
    "                                    round(np.mean(results['test_precision']),4),\n",
    "                                    round(np.mean(results['test_accuracy']), 4))]\n",
    "            else:\n",
    "                seed_results[seed].append((round(np.mean(results['test_recall']), 4),\n",
    "                                    round(np.mean(results['test_precision']),4),\n",
    "                                    round(np.mean(results['test_accuracy']), 4)))\n",
    "            \n",
    "            print()\n",
    "            print(\"-- Final Model Train & Eval --\")\n",
    "            final_model = models[i]\n",
    "            final_model = model_fitting(final_model, X_train, y_train)\n",
    "            model_eval(final_model, X_test, y_test)\n",
    "            print()\n",
    "\n",
    "print(seed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Food adj mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight3_foodsum = pd.read_csv(\"/home/user18/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data/0922_3way_data/3way_basic_food_adjusted_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset  onset_tight\n",
      "0      0              55569\n",
      "1      0               1137\n",
      "       1                525\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tight3_foodsum[['onset', 'onset_tight']].value_counts())\n",
    "\n",
    "# Urine protein 1+ & eGFR >= 60 인 환자 (1137명) 2로 labeling\n",
    "tight3_foodsum.loc[(tight3_foodsum['onset'] == 1) & (tight3_foodsum['onset_tight'] == 0), 'onset_tight'] = 2\n",
    "tight3_foodsum['onset_tight'].value_counts()\n",
    "\n",
    "tight3_foodsum.drop(['onset'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.54761905 0.58333333 0.58730159 0.60714286 0.56746032]\n",
      "각 폴드의 Precision: [0.53706246 0.57842919 0.59262646 0.59193644 0.56529791]\n",
      "각 폴드의 Accuracy: [0.54761905 0.58333333 0.58730159 0.60714286 0.56746032]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5786, 0.5731, 0.5786\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5873 0.5881 0.5873\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.56349206 0.59920635 0.58333333 0.59126984 0.58333333]\n",
      "각 폴드의 Precision: [0.55572836 0.59387374 0.5710728  0.57607691 0.57031626]\n",
      "각 폴드의 Accuracy: [0.56349206 0.59920635 0.58333333 0.59126984 0.58333333]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5841, 0.5734, 0.5841\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5778 0.5639 0.5778\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.5515873  0.59920635 0.57936508 0.59920635 0.57539683]\n",
      "각 폴드의 Precision: [0.54647596 0.59760621 0.57758128 0.58103599 0.56666667]\n",
      "각 폴드의 Accuracy: [0.5515873  0.59920635 0.57936508 0.59920635 0.57539683]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.581, 0.5739, 0.581\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5746 0.5699 0.5746\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=42) in fold 5, with seed 42\n",
      "각 폴드의 Recall: [0.53571429 0.51190476 0.6031746  0.56349206 0.56349206]\n",
      "각 폴드의 Precision: [0.53399196 0.49925517 0.60361608 0.54907519 0.54336839]\n",
      "각 폴드의 Accuracy: [0.53571429 0.51190476 0.6031746  0.56349206 0.56349206]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5556, 0.5459, 0.5556\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5238 0.5218 0.5238\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.58333333 0.59126984 0.57539683 0.52777778 0.6031746 ]\n",
      "각 폴드의 Precision: [0.56904912 0.58438302 0.56953699 0.51688224 0.5886798 ]\n",
      "각 폴드의 Accuracy: [0.58333333 0.59126984 0.57539683 0.52777778 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5762, 0.5657, 0.5762\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5746 0.563 0.5746\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.57142857 0.57142857 0.57142857 0.5        0.52380952]\n",
      "각 폴드의 Precision: [0.556232   0.55587801 0.556505   0.47561591 0.48962639]\n",
      "각 폴드의 Accuracy: [0.57142857 0.57142857 0.57142857 0.5        0.52380952]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5476, 0.5268, 0.5476\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.546 0.516 0.546\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.5515873  0.59126984 0.57142857 0.50793651 0.59126984]\n",
      "각 폴드의 Precision: [0.53126458 0.57829457 0.56175806 0.49344159 0.58249158]\n",
      "각 폴드의 Accuracy: [0.5515873  0.59126984 0.57142857 0.50793651 0.59126984]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5627, 0.5495, 0.5627\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5651 0.5585 0.5651\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=59) in fold 5, with seed 59\n",
      "각 폴드의 Recall: [0.49603175 0.56349206 0.56349206 0.53174603 0.55555556]\n",
      "각 폴드의 Precision: [0.48106629 0.53846862 0.55089928 0.53113556 0.55349881]\n",
      "각 폴드의 Accuracy: [0.49603175 0.56349206 0.56349206 0.53174603 0.55555556]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5421, 0.531, 0.5421\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5492 0.5514 0.5492\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.5515873  0.57142857 0.64285714 0.56349206 0.58730159]\n",
      "각 폴드의 Precision: [0.54328272 0.56286654 0.63330314 0.55752916 0.58204607]\n",
      "각 폴드의 Accuracy: [0.5515873  0.57142857 0.64285714 0.56349206 0.58730159]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5758, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5714 0.5723 0.5714\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.58333333 0.53968254 0.55555556 0.56349206 0.55952381]\n",
      "각 폴드의 Precision: [0.56672953 0.52024533 0.5379989  0.54747135 0.53331452]\n",
      "각 폴드의 Accuracy: [0.58333333 0.53968254 0.55555556 0.56349206 0.55952381]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5603, 0.5412, 0.5603\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5524 0.5332 0.5524\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.54761905 0.57142857 0.62698413 0.57936508 0.57936508]\n",
      "각 폴드의 Precision: [0.53819452 0.55701121 0.61389641 0.57154255 0.5755814 ]\n",
      "각 폴드의 Accuracy: [0.54761905 0.57142857 0.62698413 0.57936508 0.57936508]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.581, 0.5712, 0.581\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5651 0.5623 0.5651\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=63) in fold 5, with seed 63\n",
      "각 폴드의 Recall: [0.52380952 0.57539683 0.50396825 0.52380952 0.57142857]\n",
      "각 폴드의 Precision: [0.51611268 0.55827406 0.48304919 0.50631045 0.55861432]\n",
      "각 폴드의 Accuracy: [0.52380952 0.57539683 0.50396825 0.52380952 0.57142857]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5397, 0.5245, 0.5397\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5333 0.5179 0.5333\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.58730159 0.57142857 0.58730159 0.5515873  0.61904762]\n",
      "각 폴드의 Precision: [0.5734799  0.5584453  0.58113878 0.5585093  0.62374612]\n",
      "각 폴드의 Accuracy: [0.58730159 0.57142857 0.58730159 0.5515873  0.61904762]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5791, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.546 0.5394 0.546\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.57936508 0.57936508 0.57142857 0.56349206 0.61507937]\n",
      "각 폴드의 Precision: [0.55772351 0.55782828 0.56209194 0.55071352 0.60513664]\n",
      "각 폴드의 Accuracy: [0.57936508 0.57936508 0.57142857 0.56349206 0.61507937]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5817, 0.5667, 0.5817\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5206 0.5037 0.5206\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.59126984 0.58333333 0.58333333 0.55952381 0.63095238]\n",
      "각 폴드의 Precision: [0.57117595 0.5712579  0.57655784 0.55589509 0.63052326]\n",
      "각 폴드의 Accuracy: [0.59126984 0.58333333 0.58333333 0.55952381 0.63095238]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5897, 0.5811, 0.5897\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5397 0.533 0.5397\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=79) in fold 5, with seed 79\n",
      "각 폴드의 Recall: [0.54761905 0.54365079 0.56746032 0.52777778 0.54761905]\n",
      "각 폴드의 Precision: [0.5459197  0.51616526 0.56750162 0.54735252 0.55887659]\n",
      "각 폴드의 Accuracy: [0.54761905 0.54365079 0.56746032 0.52777778 0.54761905]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5468, 0.5472, 0.5468\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5079 0.5203 0.5079\n",
      "\n",
      "For undersampling dataset!!!\n",
      "onset_tight\n",
      "0    420\n",
      "1    420\n",
      "2    420\n",
      "Name: count, dtype: int64\n",
      "undersampling dataset loaded and scaled\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.53174603 0.61507937 0.61904762 0.54761905 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52473151 0.61265179 0.61278999 0.537072   0.59647153]\n",
      "각 폴드의 Accuracy: [0.53174603 0.61507937 0.61904762 0.54761905 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5833, 0.5767, 0.5833\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5587 0.5652 0.5587\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.57539683 0.55555556 0.60714286 0.54365079 0.56746032]\n",
      "각 폴드의 Precision: [0.56668044 0.54452402 0.59427609 0.52225768 0.55557434]\n",
      "각 폴드의 Accuracy: [0.57539683 0.55555556 0.60714286 0.54365079 0.56746032]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5698, 0.5567, 0.5698\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5619 0.5489 0.5619\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.55952381 0.61111111 0.61904762 0.53571429 0.62301587]\n",
      "각 폴드의 Precision: [0.54741066 0.60660194 0.60883236 0.52690694 0.61914539]\n",
      "각 폴드의 Accuracy: [0.55952381 0.61111111 0.61904762 0.53571429 0.62301587]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.5897, 0.5818, 0.5897\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5873 0.5863 0.5873\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=101) in fold 5, with seed 101\n",
      "각 폴드의 Recall: [0.52777778 0.56349206 0.55952381 0.51587302 0.6031746 ]\n",
      "각 폴드의 Precision: [0.52269485 0.55818007 0.55373058 0.50468681 0.64703602]\n",
      "각 폴드의 Accuracy: [0.52777778 0.56349206 0.55952381 0.51587302 0.6031746 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.554, 0.5573, 0.554\n",
      "\n",
      "-- Final Model Train & Eval --\n",
      "Recall \t Precision \t Acc\n",
      "0.5333 0.528 0.5333\n",
      "\n",
      "{42: [(0.5786, 0.5731, 0.5786), (0.5841, 0.5734, 0.5841), (0.581, 0.5739, 0.581), (0.5556, 0.5459, 0.5556)], 59: [(0.5762, 0.5657, 0.5762), (0.5476, 0.5268, 0.5476), (0.5627, 0.5495, 0.5627), (0.5421, 0.531, 0.5421)], 63: [(0.5833, 0.5758, 0.5833), (0.5603, 0.5412, 0.5603), (0.581, 0.5712, 0.581), (0.5397, 0.5245, 0.5397)], 79: [(0.5833, 0.5791, 0.5833), (0.5817, 0.5667, 0.5817), (0.5897, 0.5811, 0.5897), (0.5468, 0.5472, 0.5468)], 101: [(0.5833, 0.5767, 0.5833), (0.5698, 0.5567, 0.5698), (0.5897, 0.5818, 0.5897), (0.554, 0.5573, 0.554)]}\n"
     ]
    }
   ],
   "source": [
    "n_split = 5\n",
    "\n",
    "# Train/test divide => Always Same\n",
    "tight3_foodsum_train, tight3_foodsum_test = divide_testset(tight3_foodsum, 0.2)        # train-test ratio\n",
    "y_tight3_test = tight3_foodsum_test['onset_tight']\n",
    "X_tight3_test = tight3_foodsum_test.drop(['RID', 'onset_tight'], axis=1)\n",
    "\n",
    "seed_results = {}\n",
    "for seed in [42, 59, 63, 79, 101]:    \n",
    "    datas = {\"original\":(), \"undersampling\":(), \"oversampling\":(), \"test\":(X_tight3_test, y_tight3_test)}\n",
    "\n",
    "    datas['undersampling'] = undersampling(tight3_foodsum_train, seed=seed)\n",
    "    datas['oversampling'] = oversampling(tight3_foodsum_train, seed=seed)\n",
    "    y_tight3_foodsum_train = tight3_foodsum_train['onset_tight']\n",
    "    X_tight3_foodsum_train = tight3_foodsum_train.drop(['RID', 'onset_tight'], axis=1)\n",
    "    datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "    \n",
    "    for data in ['undersampling']:\n",
    "        print(f\"For {data} dataset!!!\")\n",
    "        X_test, y_test = datas['test']\n",
    "        X_train, y_train = datas[data]\n",
    "        print(y_train.value_counts())\n",
    "        \n",
    "        wei_train_scaler = StandardScaler()\n",
    "        X_train = wei_train_scaler.fit_transform(X_train)\n",
    "        X_test = wei_train_scaler.transform(X_test)\n",
    "        print(f\"{data} dataset loaded and scaled\")\n",
    "        \n",
    "        scoring = {\n",
    "        'recall': 'recall_macro',      # recall for each class, then averaged\n",
    "        'precision': 'precision_macro',# precision for each class, then averaged\n",
    "        'accuracy': 'accuracy',        # accuracy\n",
    "        # 'auc': make_scorer(roc_auc_score, multi_class='ovr')  # AUC 계산 (이진 분류시)\n",
    "        }\n",
    "        \n",
    "        models = (\n",
    "            SVC(kernel='linear', random_state=seed, probability=True),\n",
    "            RandomForestClassifier(random_state=seed, max_depth=3),\n",
    "            LogisticRegression(max_iter=1000, random_state=seed),\n",
    "            GradientBoostingClassifier(random_state=seed, max_depth=1, learning_rate=0.01),\n",
    "        )\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            print(\"=\" * 30)\n",
    "            print(f\"Cross Validation for {model} in fold {n_split}, with seed {seed}\")\n",
    "            # # Stratified K-Fold 교차 검증 (K=5)\n",
    "            skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "            results = cross_validate(model, X_train, y_train, cv=skf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            # 결과 출력\n",
    "            print(\"각 폴드의 Recall:\", results['test_recall'])\n",
    "            print(\"각 폴드의 Precision:\", results['test_precision'])\n",
    "            print(\"각 폴드의 Accuracy:\", results['test_accuracy'])\n",
    "            # print(\"각 폴드의 AUC:\", results['test_auc'])\n",
    "\n",
    "            # 평균값 계산\n",
    "            print(\"평균 Recall, Precision, Accuracy, AUC:\")\n",
    "            print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}\")\n",
    "            # print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}, {round(np.mean(results['test_auc']), 4)}\")\n",
    "            \n",
    "            if seed not in seed_results.keys():\n",
    "                seed_results[seed] = [(round(np.mean(results['test_recall']), 4),\n",
    "                                    round(np.mean(results['test_precision']),4),\n",
    "                                    round(np.mean(results['test_accuracy']), 4))]\n",
    "            else:\n",
    "                seed_results[seed].append((round(np.mean(results['test_recall']), 4),\n",
    "                                    round(np.mean(results['test_precision']),4),\n",
    "                                    round(np.mean(results['test_accuracy']), 4)))\n",
    "            \n",
    "            print()\n",
    "            print(\"-- Final Model Train & Eval --\")\n",
    "            final_model = models[i]\n",
    "            final_model = model_fitting(final_model, X_train, y_train)\n",
    "            model_eval(final_model, X_test, y_test)\n",
    "            print()\n",
    "\n",
    "print(seed_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
