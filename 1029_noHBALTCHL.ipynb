{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, recall_score, precision_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import os\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import pickle\n",
    "\n",
    "# ConvergenceWarning 경고를 무시\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FUNCTIONS\"\"\"\n",
    "\n",
    "# test에 나머지 control sample 추가해서 idx 만 반환\n",
    "def divide_testset(unbalanced_data, ratio):\n",
    "    # train에서 ckd, control index 확인\n",
    "    total_idx = unbalanced_data.index\n",
    "    ckd_idx = unbalanced_data[unbalanced_data['onset_tight'] == 1].index        # 실제 ckd\n",
    "    control_idx = unbalanced_data[unbalanced_data['onset_tight'] == 0].index    # 실제 control\n",
    "    # print(control_idx, ckd_idx)\n",
    "\n",
    "    # ckd 갯수와 동일하게 control idx sampling\n",
    "    rng = np.random.default_rng(seed=0) \n",
    "    sampled_ckd_idx = pd.Index(rng.choice(ckd_idx, size=int(len(ckd_idx)*ratio), replace=False))\n",
    "    sampled_control_idx = pd.Index(rng.choice(control_idx, size=len(sampled_ckd_idx), replace=False)) # test_ckd 갯수와 동일하게 sampling\n",
    "    \n",
    "    test_idx = sampled_ckd_idx.append(sampled_control_idx)\n",
    "    train_idx = total_idx.difference(test_idx)\n",
    "\n",
    "    # return 실제 ckd, 실제 ckd 갯수와 동일한 갯수의 subject, control_idx - ckd_idx\n",
    "    return unbalanced_data.loc[train_idx], unbalanced_data.loc[test_idx]\n",
    "\n",
    "### Oversampling\n",
    "def oversampling(unbalanced_dataframe, seed):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote = SMOTE(random_state=seed)\n",
    "    temp = unbalanced_dataframe.drop(['RID'], axis=1)\n",
    "    X_train, y_train = smote.fit_resample(temp, temp['onset_tight'])\n",
    "\n",
    "    # X_train에는 RID, onset_3 없음.\n",
    "    return X_train.drop(['onset_tight'], axis=1), y_train\n",
    "\n",
    "### Undersampling\n",
    "# test에 나머지 control sample 추가해서 idx 만 반환\n",
    "def _under_sampling_idx(unbalanced_data, seed):\n",
    "    # train에서 ckd, control index 확인\n",
    "    ckd_idx = unbalanced_data[unbalanced_data['onset_tight'] == 1].index        # 실제 ckd\n",
    "    control_idx = unbalanced_data[unbalanced_data['onset_tight'] == 0].index    # 실제 control\n",
    "    # print(control_idx, ckd_idx)\n",
    "\n",
    "    # ckd 갯수와 동일하게 control idx sampling\n",
    "    \"\"\"\n",
    "    Control CKD 비율 조정\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed=seed) \n",
    "    sampled_control_idx = pd.Index(rng.choice(control_idx, size=len(ckd_idx), replace=False)) # ckd 갯수와 동일하게 sampling\n",
    "    not_sampled_control_idx = control_idx.difference(sampled_control_idx)\n",
    "\n",
    "    # 잘 sampling 되었는지 확인\n",
    "    assert set(sampled_control_idx).issubset(set(control_idx))\n",
    "    # print(len(sampled_control_idx))\n",
    "\n",
    "    balanced_idx = sampled_control_idx.append(ckd_idx)\n",
    "\n",
    "    # return 실제 ckd, 실제 ckd 갯수와 동일한 갯수의 subject, control_idx - ckd_idx\n",
    "    return ckd_idx, sampled_control_idx, not_sampled_control_idx, balanced_idx\n",
    "\n",
    "def undersampling(unbalanced_data, seed):\n",
    "    a, b, c, d = _under_sampling_idx(unbalanced_data, seed)\n",
    "    under_sampled_data = unbalanced_data.loc[d]\n",
    "    X_undersampled = under_sampled_data.drop(['RID', 'onset_tight'], axis=1)\n",
    "    y_undersampled = under_sampled_data['onset_tight']\n",
    "    return X_undersampled, y_undersampled\n",
    "\n",
    "def print_cv_results_ML(results_all_seed):\n",
    "    # 모델별로 시드들의 평균과 표준편차를 구하는 부분\n",
    "    results_summary = {}\n",
    "\n",
    "    # 각 시드에 저장된 모델별 결과에 접근\n",
    "    for seed in results_all_seed:\n",
    "        for model_name in results_all_seed[seed]:\n",
    "            if model_name not in results_summary:\n",
    "                results_summary[model_name] = {\n",
    "                    'recall': [],\n",
    "                    'precision': [],\n",
    "                    'accuracy': [],\n",
    "                    'auc': [],\n",
    "                    'recall_std': [],\n",
    "                    'precision_std': [],\n",
    "                    'accuracy_std': [],\n",
    "                    'auc_std': []\n",
    "                }\n",
    "            # 각 시드의 성능 값을 추가\n",
    "            results_summary[model_name]['recall'].append(results_all_seed[seed][model_name]['recall'][0])\n",
    "            results_summary[model_name]['precision'].append(results_all_seed[seed][model_name]['precision'][0])\n",
    "            results_summary[model_name]['accuracy'].append(results_all_seed[seed][model_name]['accuracy'][0])\n",
    "            results_summary[model_name]['auc'].append(results_all_seed[seed][model_name]['auc'][0])\n",
    "            results_summary[model_name]['recall_std'].append(results_all_seed[seed][model_name]['recall_std'][0])\n",
    "            results_summary[model_name]['precision_std'].append(results_all_seed[seed][model_name]['precision_std'][0])\n",
    "            results_summary[model_name]['accuracy_std'].append(results_all_seed[seed][model_name]['accuracy_std'][0])\n",
    "            results_summary[model_name]['auc_std'].append(results_all_seed[seed][model_name]['auc_std'][0])\n",
    "\n",
    "    # 모델별로 평균과 표준편차 계산\n",
    "    for model_name in results_summary:\n",
    "        print(f\"Model: {model_name}\")\n",
    "        \n",
    "        recall_mean = np.mean(results_summary[model_name]['recall'])\n",
    "        recall_std = np.mean(results_summary[model_name]['recall_std'])\n",
    "        \n",
    "        precision_mean = np.mean(results_summary[model_name]['precision'])\n",
    "        precision_std = np.mean(results_summary[model_name]['precision_std'])\n",
    "        \n",
    "        accuracy_mean = np.mean(results_summary[model_name]['accuracy'])\n",
    "        accuracy_std = np.mean(results_summary[model_name]['accuracy_std'])\n",
    "        \n",
    "        auc_mean = np.mean(results_summary[model_name]['auc'])\n",
    "        auc_std = np.mean(results_summary[model_name]['auc_std'])\n",
    "        \n",
    "        print(f\"Recall Precision Accuracy AUC\")\n",
    "        print(f\"{recall_mean:.4f}, {precision_mean:.4f}, {accuracy_mean:.4f}, {auc_mean:.4f}\")\n",
    "        print(f\"{recall_std:.4f}, {precision_std:.4f}, {accuracy_std:.4f}, {auc_std:.4f}\")    \n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "def get_results(y_test, final_prediction, final_probabilities):\n",
    "    cm = confusion_matrix(list(y_test), list(final_prediction))\n",
    "    print(cm)\n",
    "    tn, fn, tp, fp  = cm[0][0], cm[1][0], cm[1][1], cm[0][1]\n",
    "    recall = tp / (fn + tp)\n",
    "    precision = tp / (fp + tp)\n",
    "    acc = (tp + tn) / (tn + fn + tp + fp)\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, final_probabilities)\n",
    "    \n",
    "    print(\"Recall \\t Precision \\t Acc \\t AUC\")\n",
    "    print(f\"{np.round(recall, 4)} {np.round(precision, 4)} {np.round(acc, 4)} {np.round(auc, 4)}\")\n",
    "    \n",
    "def model_fitting(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def model_eval(fitted_model, X_test, y_test):\n",
    "    model_prediction = fitted_model.predict(X_test)\n",
    "    model_probabilities = fitted_model.predict_proba(X_test)[:, 1]\n",
    "    model_score = fitted_model.score(X_test, y_test)\n",
    "    print(f\"Score with simple {fitted_model} model\")\n",
    "    print(0.5, np.round(model_score, 4))     # accuracy\n",
    "\n",
    "    get_results(y_test, model_prediction, model_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Adjusted mean food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/user19/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/data/0922_data\"\n",
    "total = pd.read_csv(f\"{root_path}/0922_basic_food_adjusted_mean.csv\")\n",
    "basic = pd.read_csv(f\"{root_path}/0922_basic_only.csv\")\n",
    "food = pd.read_csv(f\"{root_path}/0922_food_adjusted_mean_only.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No HB, ALBUMIN, TCHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbaltchl = ['CT1_HB', 'Imp_CT1_ALBUMIN', 'CT1_TCHL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_no_hb = total.drop(hbaltchl, axis=1)\n",
    "basic_no_hb = basic.drop(hbaltchl, axis=1)\n",
    "# food_no_hb = food.drop(hbaltchl, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (1050, 12), y_train :: (1050,)\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=42) in fold 10, with seed 42\n",
      "각 폴드의 Recall: [0.80986938 0.85703919 0.77231495 0.78120464 0.80025399 0.94285196\n",
      " 0.84760522 0.88588534 0.82855588 0.82928157]\n",
      "각 폴드의 Precision: [0.81122449 0.85729847 0.78150788 0.78181818 0.80090909 0.94285196\n",
      " 0.84760522 0.88616558 0.82855588 0.83611111]\n",
      "각 폴드의 Accuracy: [0.80952381 0.85714286 0.77142857 0.78095238 0.8        0.94285714\n",
      " 0.84761905 0.88571429 0.82857143 0.82857143]\n",
      "각 폴드의 AUC: [0.80986938 0.85703919 0.77231495 0.78120464 0.80025399 0.94285196\n",
      " 0.84760522 0.88588534 0.82855588 0.82928157]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8355, 0.8374, 0.8352, 0.8355\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0486, 0.0472, 0.0488, 0.0486\n",
      "\n",
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (1050, 12), y_train :: (1050,)\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=59) in fold 10, with seed 59\n",
      "각 폴드의 Recall: [0.77068215 0.82801161 0.81857765 0.79082003 0.84760522 0.85740203\n",
      " 0.83835269 0.77158926 0.90493469 0.82891872]\n",
      "각 폴드의 Precision: [0.77794337 0.83308769 0.82208364 0.79209184 0.84760522 0.85818182\n",
      " 0.83909091 0.77178649 0.90522876 0.83035714]\n",
      "각 폴드의 Accuracy: [0.77142857 0.82857143 0.81904762 0.79047619 0.84761905 0.85714286\n",
      " 0.83809524 0.77142857 0.9047619  0.82857143]\n",
      "각 폴드의 AUC: [0.77068215 0.82801161 0.81857765 0.79082003 0.84760522 0.85740203\n",
      " 0.83835269 0.77158926 0.90493469 0.82891872]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8257, 0.8277, 0.8257, 0.8257\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0389, 0.038, 0.0388, 0.0389\n",
      "\n",
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (1050, 12), y_train :: (1050,)\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=63) in fold 10, with seed 63\n",
      "각 폴드의 Recall: [0.837627   0.84814949 0.78102322 0.76197388 0.82837446 0.86629173\n",
      " 0.79916546 0.78102322 0.79063861 0.87590711]\n",
      "각 폴드의 Precision: [0.84134263 0.85198092 0.78102322 0.76197388 0.82909091 0.8689693\n",
      " 0.80926482 0.78102322 0.79084967 0.87755102]\n",
      "각 폴드의 Accuracy: [0.83809524 0.84761905 0.78095238 0.76190476 0.82857143 0.86666667\n",
      " 0.8        0.78095238 0.79047619 0.87619048]\n",
      "각 폴드의 AUC: [0.837627   0.84814949 0.78102322 0.76197388 0.82837446 0.86629173\n",
      " 0.79916546 0.78102322 0.79063861 0.87590711]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.817, 0.8193, 0.8171, 0.817\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0375, 0.0383, 0.0376, 0.0375\n",
      "\n",
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (1050, 12), y_train :: (1050,)\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=79) in fold 10, with seed 79\n",
      "각 폴드의 Recall: [0.78193033 0.78047896 0.79045718 0.76288099 0.84760522 0.77086357\n",
      " 0.81894049 0.80914369 0.82837446 0.83780842]\n",
      "각 폴드의 Precision: [0.79365079 0.78356566 0.79045718 0.77380952 0.84760522 0.77505527\n",
      " 0.81917211 0.81140351 0.82909091 0.83928571]\n",
      "각 폴드의 Accuracy: [0.78095238 0.78095238 0.79047619 0.76190476 0.84761905 0.77142857\n",
      " 0.81904762 0.80952381 0.82857143 0.83809524]\n",
      "각 폴드의 AUC: [0.78193033 0.78047896 0.79045718 0.76288099 0.84760522 0.77086357\n",
      " 0.81894049 0.80914369 0.82837446 0.83780842]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8028, 0.8063, 0.8029, 0.8028\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0281, 0.0254, 0.0283, 0.0281\n",
      "\n",
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (1050, 12), y_train :: (1050,)\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=101) in fold 10, with seed 101\n",
      "각 폴드의 Recall: [0.82837446 0.80968795 0.85703919 0.80986938 0.8474238  0.79989115\n",
      " 0.87572569 0.80968795 0.79989115 0.80986938]\n",
      "각 폴드의 Precision: [0.82909091 0.80991285 0.85729847 0.81122449 0.84818182 0.80010893\n",
      " 0.8798606  0.80991285 0.80010893 0.81122449]\n",
      "각 폴드의 Accuracy: [0.82857143 0.80952381 0.85714286 0.80952381 0.84761905 0.8\n",
      " 0.87619048 0.80952381 0.8        0.80952381]\n",
      "각 폴드의 AUC: [0.82837446 0.80968795 0.85703919 0.80986938 0.8474238  0.79989115\n",
      " 0.87572569 0.80968795 0.79989115 0.80986938]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8247, 0.8257, 0.8248, 0.8247\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0251, 0.0258, 0.0253, 0.0251\n",
      "\n",
      "Model: GradientBoostingClassifier\n",
      "Recall Precision Accuracy AUC\n",
      "0.8212, 0.8233, 0.8211, 0.8212\n",
      "0.0356, 0.0349, 0.0357, 0.0356\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Total Dataset ( Food + Basic ) \"\"\"\n",
    "\n",
    "results_all_seed = {}\n",
    "\n",
    "n_split = 10\n",
    "for seed in [42, 59, 63, 79, 101]:    \n",
    "    datas = {\"original\":(), \"undersampling\":(), \"oversampling\":()}\n",
    "\n",
    "    datas['undersampling'] = undersampling(basic_no_hb, seed=seed)        # eGFR 만을 빼면 성능이 줄어든다..\n",
    "    datas['oversampling'] = oversampling(basic_no_hb, seed=seed)\n",
    "    y_tight3_foodsum_train = basic_no_hb['onset_tight']\n",
    "    X_tight3_foodsum_train = basic_no_hb.drop(['RID', 'onset_tight'], axis=1)\n",
    "    datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "    \n",
    "    results_all_seed[seed] = {}  # 각 시드마다 딕셔너리 생성\n",
    "    \n",
    "    for data in ['undersampling']:\n",
    "        print(f\"For {data} dataset!!!\")\n",
    "        # X_test, y_test = datas['test']\n",
    "        X_train, y_train = datas[data]\n",
    "        \n",
    "        wei_train_scaler = StandardScaler()\n",
    "        X_train = wei_train_scaler.fit_transform(X_train)\n",
    "        # X_test = wei_train_scaler.transform(X_test)\n",
    "        print(f\"{data} dataset loaded and scaled\")\n",
    "        print(f\"X_train :: {X_train.shape}, y_train :: {y_train.shape}\")\n",
    "        \n",
    "        scoring = {\n",
    "        'recall': 'recall_macro',      # recall for each class, then averaged\n",
    "        'precision': 'precision_macro',# precision for each class, then averaged\n",
    "        'accuracy': 'accuracy',        # accuracy\n",
    "        'auc': make_scorer(roc_auc_score, multi_class='ovr')  # AUC 계산 (이진 분류시)\n",
    "        }\n",
    "        \n",
    "        models = (\n",
    "            # SVC(kernel='linear', random_state=seed, probability=True),\n",
    "            # RandomForestClassifier(random_state=seed, max_depth=3),\n",
    "            # LogisticRegression(max_iter=1000, random_state=seed),\n",
    "            GradientBoostingClassifier(random_state=seed, max_depth=3, learning_rate=0.01),\n",
    "            # xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=seed)\n",
    "        )\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            model_name = model.__class__.__name__  # 모델 이름 저장\n",
    "            print(\"=\" * 30)\n",
    "            print(f\"Cross Validation for {model} in fold {n_split}, with seed {seed}\")\n",
    "            # Stratified K-Fold 교차 검증 (K=5)\n",
    "            skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "            results = cross_validate(model, X_train, y_train, cv=skf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            # 결과 출력\n",
    "            print(\"각 폴드의 Recall:\", results['test_recall'])\n",
    "            print(\"각 폴드의 Precision:\", results['test_precision'])\n",
    "            print(\"각 폴드의 Accuracy:\", results['test_accuracy'])\n",
    "            print(\"각 폴드의 AUC:\", results['test_auc'])\n",
    "\n",
    "            # 결과 저장\n",
    "            if model_name not in results_all_seed[seed]:\n",
    "                results_all_seed[seed][model_name] = {\n",
    "                    'recall': [], \n",
    "                    'precision': [], \n",
    "                    'accuracy': [], \n",
    "                    'auc': [],\n",
    "                    'recall_std': [], \n",
    "                    'precision_std': [], \n",
    "                    'accuracy_std': [], \n",
    "                    'auc_std': []\n",
    "                }\n",
    "            results_all_seed[seed][model_name]['recall'].append(np.mean(results['test_recall']))\n",
    "            results_all_seed[seed][model_name]['precision'].append(np.mean(results['test_precision']))\n",
    "            results_all_seed[seed][model_name]['accuracy'].append(np.mean(results['test_accuracy']))\n",
    "            results_all_seed[seed][model_name]['auc'].append(np.mean(results['test_auc']))\n",
    "            results_all_seed[seed][model_name]['recall_std'].append(np.std(results['test_recall']))\n",
    "            results_all_seed[seed][model_name]['precision_std'].append(np.std(results['test_precision']))\n",
    "            results_all_seed[seed][model_name]['accuracy_std'].append(np.std(results['test_accuracy']))\n",
    "            results_all_seed[seed][model_name]['auc_std'].append(np.std(results['test_auc']))\n",
    "            \n",
    "            \n",
    "            # 평균값 계산\n",
    "            print(\"평균 Recall, Precision, Accuracy, AUC:\")\n",
    "            print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}, {round(np.mean(results['test_auc']), 4)}\")\n",
    "            print(\"표준편차 Recall, Precision, Accuracy, AUC:\")\n",
    "            print(f\"{round(np.std(results['test_recall']), 4)}, {round(np.std(results['test_precision']),4)}, {round(np.std(results['test_accuracy']), 4)}, {round(np.std(results['test_auc']), 4)}\")\n",
    "            print()\n",
    "            \n",
    "print_cv_results_ML(results_all_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 기저질환 환자 명수 확인\n",
    "\n",
    "- 기저질환이 모두 없는 환자 : 37943 ( 약 66.30% )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_CT1_gohyeol  CT1_dangnyo  CT1_simhyeol\n",
      "0                0            0               37943\n",
      "1                0            0               12542\n",
      "0                1            0                2382\n",
      "1                1            0                2239\n",
      "                 0            1                 844\n",
      "0                0            1                 836\n",
      "1                1            1                 278\n",
      "0                1            1                 167\n",
      "Name: count, dtype: int64\n",
      "new_CT1_gohyeol  CT1_dangnyo  CT1_simhyeol\n",
      "0                0            0               66.297985\n",
      "1                0            0               21.914697\n",
      "0                1            0                4.162080\n",
      "1                1            0                3.912215\n",
      "                 0            1                1.474725\n",
      "0                0            1                1.460747\n",
      "1                1            1                0.485751\n",
      "0                1            1                0.291800\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "col_giju = ['new_CT1_gohyeol', 'CT1_dangnyo', 'CT1_simhyeol']\n",
    "print(total[col_giju].value_counts())\n",
    "print((total[col_giju].value_counts() / total[col_giju].value_counts().sum()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기저 질환이 모두 없는 환자 ( 37943명 중에서 CKD 발생이 몇 명인지 확인)\n",
    "- 37943 명 중 Control : 37814 명, CKD 129 명 -> (CKD 비율 0.3 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_jilhwan_total = total[(total['new_CT1_gohyeol'] == 0) & (total['CT1_dangnyo'] == 0) & (total['CT1_simhyeol'] == 0)]\n",
    "no_jilhwan_basic = basic[(total['new_CT1_gohyeol'] == 0) & (total['CT1_dangnyo'] == 0) & (total['CT1_simhyeol'] == 0)]\n",
    "\n",
    "# 고혈압, 당뇨, 심혈관질환 컬럼 제외\n",
    "no_jilhwan_total = no_jilhwan_total.drop(col_giju, axis=1)      # 12 + 21\n",
    "no_jilhwan_basic = no_jilhwan_basic.drop(col_giju, axis=1)      # 12\n",
    "\n",
    "# 추가로 TC, albumin, HB 컬럼 제외\n",
    "no_jilhwan_total_no_hb = no_jilhwan_total.drop(hbaltchl, axis=1)        # 12 + 21 - 3\n",
    "no_jilhwan_basic_no_hb = no_jilhwan_basic.drop(hbaltchl, axis=1)        # 12 -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset_tight\n",
      "0    37814\n",
      "1      129\n",
      "Name: count, dtype: int64\n",
      "onset_tight\n",
      "0    99.660016\n",
      "1     0.339984\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(no_jilhwan_basic['onset_tight'].value_counts())\n",
    "print((no_jilhwan_total['onset_tight'].value_counts() / no_jilhwan_total['onset_tight'].value_counts().sum())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (258, 9), y_train :: (258,)\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=42) in fold 10, with seed 42\n",
      "각 폴드의 Recall: [0.92307692 0.76923077 0.80769231 0.80769231 0.88461538 0.84615385\n",
      " 0.84615385 0.76923077 0.79807692 0.83974359]\n",
      "각 폴드의 Precision: [0.93333333 0.77575758 0.80952381 0.80952381 0.88690476 0.88235294\n",
      " 0.88235294 0.77575758 0.80194805 0.83974359]\n",
      "각 폴드의 Accuracy: [0.92307692 0.76923077 0.80769231 0.80769231 0.88461538 0.84615385\n",
      " 0.84615385 0.76923077 0.8        0.84      ]\n",
      "각 폴드의 AUC: [0.92307692 0.76923077 0.80769231 0.80769231 0.88461538 0.84615385\n",
      " 0.84615385 0.76923077 0.79807692 0.83974359]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8292, 0.8397, 0.8294, 0.8292\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0465, 0.051, 0.0464, 0.0465\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=42) in fold 10, with seed 42\n",
      "각 폴드의 Recall: [0.88461538 0.84615385 0.80769231 0.80769231 0.88461538 0.92307692\n",
      " 0.84615385 0.80769231 0.79807692 0.88141026]\n",
      "각 폴드의 Precision: [0.90625    0.88235294 0.80952381 0.80952381 0.88690476 0.93333333\n",
      " 0.88235294 0.80952381 0.80194805 0.88141026]\n",
      "각 폴드의 Accuracy: [0.88461538 0.84615385 0.80769231 0.80769231 0.88461538 0.92307692\n",
      " 0.84615385 0.80769231 0.8        0.88      ]\n",
      "각 폴드의 AUC: [0.88461538 0.84615385 0.80769231 0.80769231 0.88461538 0.92307692\n",
      " 0.84615385 0.80769231 0.79807692 0.88141026]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8487, 0.8603, 0.8488, 0.8487\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.041, 0.0455, 0.0406, 0.041\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=42) in fold 10, with seed 42\n",
      "각 폴드의 Recall: [0.92307692 0.80769231 0.80769231 0.84615385 0.84615385 0.88461538\n",
      " 0.76923077 0.76923077 0.80128205 0.83974359]\n",
      "각 폴드의 Precision: [0.93333333 0.80952381 0.80952381 0.85454545 0.84615385 0.90625\n",
      " 0.84210526 0.76923077 0.80128205 0.83974359]\n",
      "각 폴드의 Accuracy: [0.92307692 0.80769231 0.80769231 0.84615385 0.84615385 0.88461538\n",
      " 0.76923077 0.76923077 0.8        0.84      ]\n",
      "각 폴드의 AUC: [0.92307692 0.80769231 0.80769231 0.84615385 0.84615385 0.88461538\n",
      " 0.76923077 0.76923077 0.80128205 0.83974359]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8295, 0.8412, 0.8294, 0.8295\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0463, 0.0466, 0.0464, 0.0463\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=42) in fold 10, with seed 42\n",
      "각 폴드의 Recall: [0.88461538 0.80769231 0.80769231 0.80769231 0.84615385 0.88461538\n",
      " 0.88461538 0.76923077 0.79807692 0.88141026]\n",
      "각 폴드의 Precision: [0.90625    0.86111111 0.80952381 0.80952381 0.85454545 0.90625\n",
      " 0.90625    0.79738562 0.80194805 0.88141026]\n",
      "각 폴드의 Accuracy: [0.88461538 0.80769231 0.80769231 0.80769231 0.84615385 0.88461538\n",
      " 0.88461538 0.76923077 0.8        0.88      ]\n",
      "각 폴드의 AUC: [0.88461538 0.80769231 0.80769231 0.80769231 0.84615385 0.88461538\n",
      " 0.88461538 0.76923077 0.79807692 0.88141026]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8372, 0.8534, 0.8372, 0.8372\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0419, 0.0434, 0.0416, 0.0419\n",
      "\n",
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (258, 9), y_train :: (258,)\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=59) in fold 10, with seed 59\n",
      "각 폴드의 Recall: [0.76923077 0.88461538 0.80769231 0.76923077 0.73076923 0.92307692\n",
      " 0.80769231 0.88461538 0.87820513 0.72115385]\n",
      "각 폴드의 Precision: [0.79738562 0.88690476 0.80952381 0.77575758 0.74375    0.93333333\n",
      " 0.825      0.88690476 0.88311688 0.72115385]\n",
      "각 폴드의 Accuracy: [0.76923077 0.88461538 0.80769231 0.76923077 0.73076923 0.92307692\n",
      " 0.80769231 0.88461538 0.88       0.72      ]\n",
      "각 폴드의 AUC: [0.76923077 0.88461538 0.80769231 0.76923077 0.73076923 0.92307692\n",
      " 0.80769231 0.88461538 0.87820513 0.72115385]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8176, 0.8263, 0.8177, 0.8176\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0675, 0.066, 0.0678, 0.0675\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=59) in fold 10, with seed 59\n",
      "각 폴드의 Recall: [0.76923077 0.84615385 0.76923077 0.76923077 0.84615385 0.92307692\n",
      " 0.80769231 0.84615385 0.87820513 0.75961538]\n",
      "각 폴드의 Precision: [0.79738562 0.84615385 0.76923077 0.77575758 0.85454545 0.92307692\n",
      " 0.86111111 0.84615385 0.88311688 0.75961538]\n",
      "각 폴드의 Accuracy: [0.76923077 0.84615385 0.76923077 0.76923077 0.84615385 0.92307692\n",
      " 0.80769231 0.84615385 0.88       0.76      ]\n",
      "각 폴드의 AUC: [0.76923077 0.84615385 0.76923077 0.76923077 0.84615385 0.92307692\n",
      " 0.80769231 0.84615385 0.87820513 0.75961538]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8215, 0.8316, 0.8217, 0.8215\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0525, 0.0512, 0.0527, 0.0525\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=59) in fold 10, with seed 59\n",
      "각 폴드의 Recall: [0.76923077 0.84615385 0.80769231 0.76923077 0.73076923 0.92307692\n",
      " 0.76923077 0.88461538 0.87820513 0.76282051]\n",
      "각 폴드의 Precision: [0.79738562 0.84615385 0.80952381 0.77575758 0.74375    0.93333333\n",
      " 0.77575758 0.88690476 0.88311688 0.76623377]\n",
      "각 폴드의 Accuracy: [0.76923077 0.84615385 0.80769231 0.76923077 0.73076923 0.92307692\n",
      " 0.76923077 0.88461538 0.88       0.76      ]\n",
      "각 폴드의 AUC: [0.76923077 0.84615385 0.80769231 0.76923077 0.73076923 0.92307692\n",
      " 0.76923077 0.88461538 0.87820513 0.76282051]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8141, 0.8218, 0.814, 0.8141\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0614, 0.0593, 0.0618, 0.0614\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=59) in fold 10, with seed 59\n",
      "각 폴드의 Recall: [0.76923077 0.80769231 0.76923077 0.76923077 0.84615385 0.76923077\n",
      " 0.80769231 0.84615385 0.87820513 0.67948718]\n",
      "각 폴드의 Precision: [0.79738562 0.80952381 0.76923077 0.77575758 0.85454545 0.77575758\n",
      " 0.825      0.84615385 0.88311688 0.67948718]\n",
      "각 폴드의 Accuracy: [0.76923077 0.80769231 0.76923077 0.76923077 0.84615385 0.76923077\n",
      " 0.80769231 0.84615385 0.88       0.68      ]\n",
      "각 폴드의 AUC: [0.76923077 0.80769231 0.76923077 0.76923077 0.84615385 0.76923077\n",
      " 0.80769231 0.84615385 0.87820513 0.67948718]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.7942, 0.8016, 0.7945, 0.7942\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0534, 0.0542, 0.0535, 0.0534\n",
      "\n",
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (258, 9), y_train :: (258,)\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=63) in fold 10, with seed 63\n",
      "각 폴드의 Recall: [0.80769231 0.69230769 0.76923077 0.80769231 0.76923077 0.73076923\n",
      " 0.88461538 0.76923077 0.80448718 0.67628205]\n",
      "각 폴드의 Precision: [0.86111111 0.69230769 0.77575758 0.825      0.79738562 0.73214286\n",
      " 0.88690476 0.76923077 0.81666667 0.68333333]\n",
      "각 폴드의 Accuracy: [0.80769231 0.69230769 0.76923077 0.80769231 0.76923077 0.73076923\n",
      " 0.88461538 0.76923077 0.8        0.68      ]\n",
      "각 폴드의 AUC: [0.80769231 0.69230769 0.76923077 0.80769231 0.76923077 0.73076923\n",
      " 0.88461538 0.76923077 0.80448718 0.67628205]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.7712, 0.784, 0.7711, 0.7712\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0579, 0.064, 0.057, 0.0579\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=63) in fold 10, with seed 63\n",
      "각 폴드의 Recall: [0.80769231 0.69230769 0.80769231 0.88461538 0.80769231 0.76923077\n",
      " 0.84615385 0.80769231 0.84294872 0.64423077]\n",
      "각 폴드의 Precision: [0.86111111 0.6969697  0.80952381 0.88690476 0.86111111 0.76923077\n",
      " 0.84615385 0.80952381 0.8474026  0.65      ]\n",
      "각 폴드의 Accuracy: [0.80769231 0.69230769 0.80769231 0.88461538 0.80769231 0.76923077\n",
      " 0.84615385 0.80769231 0.84       0.64      ]\n",
      "각 폴드의 AUC: [0.80769231 0.69230769 0.80769231 0.88461538 0.80769231 0.76923077\n",
      " 0.84615385 0.80769231 0.84294872 0.64423077]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.791, 0.8038, 0.7903, 0.791\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0688, 0.0731, 0.0695, 0.0688\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=63) in fold 10, with seed 63\n",
      "각 폴드의 Recall: [0.80769231 0.69230769 0.76923077 0.76923077 0.76923077 0.80769231\n",
      " 0.88461538 0.76923077 0.80448718 0.67307692]\n",
      "각 폴드의 Precision: [0.86111111 0.69230769 0.77575758 0.77575758 0.76923077 0.80952381\n",
      " 0.88690476 0.76923077 0.81666667 0.69852941]\n",
      "각 폴드의 Accuracy: [0.80769231 0.69230769 0.76923077 0.76923077 0.76923077 0.80769231\n",
      " 0.88461538 0.76923077 0.8        0.68      ]\n",
      "각 폴드의 AUC: [0.80769231 0.69230769 0.76923077 0.76923077 0.76923077 0.80769231\n",
      " 0.88461538 0.76923077 0.80448718 0.67307692]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.7747, 0.7855, 0.7749, 0.7747\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0569, 0.0588, 0.0555, 0.0569\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=63) in fold 10, with seed 63\n",
      "각 폴드의 Recall: [0.80769231 0.65384615 0.80769231 0.96153846 0.80769231 0.76923077\n",
      " 0.84615385 0.84615385 0.80448718 0.6025641 ]\n",
      "각 폴드의 Precision: [0.86111111 0.6625     0.80952381 0.96428571 0.86111111 0.76923077\n",
      " 0.84615385 0.84615385 0.81666667 0.6038961 ]\n",
      "각 폴드의 Accuracy: [0.80769231 0.65384615 0.80769231 0.96153846 0.80769231 0.76923077\n",
      " 0.84615385 0.84615385 0.8        0.6       ]\n",
      "각 폴드의 AUC: [0.80769231 0.65384615 0.80769231 0.96153846 0.80769231 0.76923077\n",
      " 0.84615385 0.84615385 0.80448718 0.6025641 ]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.7907, 0.8041, 0.79, 0.7907\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0954, 0.0986, 0.0958, 0.0954\n",
      "\n",
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (258, 9), y_train :: (258,)\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=79) in fold 10, with seed 79\n",
      "각 폴드의 Recall: [0.80769231 0.76923077 0.84615385 0.80769231 0.84615385 0.84615385\n",
      " 0.84615385 0.80769231 0.83653846 0.91987179]\n",
      "각 폴드의 Precision: [0.80952381 0.77575758 0.85454545 0.825      0.84615385 0.84615385\n",
      " 0.84615385 0.80952381 0.85       0.91987179]\n",
      "각 폴드의 Accuracy: [0.80769231 0.76923077 0.84615385 0.80769231 0.84615385 0.84615385\n",
      " 0.84615385 0.80769231 0.84       0.92      ]\n",
      "각 폴드의 AUC: [0.80769231 0.76923077 0.84615385 0.80769231 0.84615385 0.84615385\n",
      " 0.84615385 0.80769231 0.83653846 0.91987179]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8333, 0.8383, 0.8337, 0.8333\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0377, 0.036, 0.0378, 0.0377\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=79) in fold 10, with seed 79\n",
      "각 폴드의 Recall: [0.76923077 0.73076923 0.96153846 0.84615385 0.84615385 0.80769231\n",
      " 0.88461538 0.76923077 0.71794872 0.88461538]\n",
      "각 폴드의 Precision: [0.77575758 0.74375    0.96428571 0.84615385 0.85454545 0.80952381\n",
      " 0.88690476 0.77575758 0.72077922 0.9       ]\n",
      "각 폴드의 Accuracy: [0.76923077 0.73076923 0.96153846 0.84615385 0.84615385 0.80769231\n",
      " 0.88461538 0.76923077 0.72       0.88      ]\n",
      "각 폴드의 AUC: [0.76923077 0.73076923 0.96153846 0.84615385 0.84615385 0.80769231\n",
      " 0.88461538 0.76923077 0.71794872 0.88461538]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8218, 0.8277, 0.8215, 0.8218\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0731, 0.0726, 0.0724, 0.0731\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=79) in fold 10, with seed 79\n",
      "각 폴드의 Recall: [0.76923077 0.76923077 0.84615385 0.80769231 0.88461538 0.80769231\n",
      " 0.80769231 0.80769231 0.79807692 0.91666667]\n",
      "각 폴드의 Precision: [0.77575758 0.77575758 0.85454545 0.825      0.88690476 0.80952381\n",
      " 0.825      0.80952381 0.80194805 0.93333333]\n",
      "각 폴드의 Accuracy: [0.76923077 0.76923077 0.84615385 0.80769231 0.88461538 0.80769231\n",
      " 0.80769231 0.80769231 0.8        0.92      ]\n",
      "각 폴드의 AUC: [0.76923077 0.76923077 0.84615385 0.80769231 0.88461538 0.80769231\n",
      " 0.80769231 0.80769231 0.79807692 0.91666667]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8215, 0.8297, 0.822, 0.8215\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0452, 0.047, 0.0458, 0.0452\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=79) in fold 10, with seed 79\n",
      "각 폴드의 Recall: [0.80769231 0.76923077 0.96153846 0.76923077 0.76923077 0.80769231\n",
      " 0.80769231 0.76923077 0.67948718 0.92307692]\n",
      "각 폴드의 Precision: [0.80952381 0.79738562 0.96428571 0.77575758 0.76923077 0.80952381\n",
      " 0.80952381 0.77575758 0.67948718 0.92857143]\n",
      "각 폴드의 Accuracy: [0.80769231 0.76923077 0.96153846 0.76923077 0.76923077 0.80769231\n",
      " 0.80769231 0.76923077 0.68       0.92      ]\n",
      "각 폴드의 AUC: [0.80769231 0.76923077 0.96153846 0.76923077 0.76923077 0.80769231\n",
      " 0.80769231 0.76923077 0.67948718 0.92307692]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8064, 0.8119, 0.8062, 0.8064\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.077, 0.0768, 0.0765, 0.077\n",
      "\n",
      "For undersampling dataset!!!\n",
      "undersampling dataset loaded and scaled\n",
      "X_train :: (258, 9), y_train :: (258,)\n",
      "==============================\n",
      "Cross Validation for SVC(kernel='linear', probability=True, random_state=101) in fold 10, with seed 101\n",
      "각 폴드의 Recall: [0.80769231 0.88461538 0.73076923 0.76923077 0.76923077 0.76923077\n",
      " 0.84615385 0.80769231 0.88141026 0.96153846]\n",
      "각 폴드의 Precision: [0.80952381 0.88690476 0.73214286 0.77575758 0.77575758 0.84210526\n",
      " 0.84615385 0.80952381 0.88141026 0.96153846]\n",
      "각 폴드의 Accuracy: [0.80769231 0.88461538 0.73076923 0.76923077 0.76923077 0.76923077\n",
      " 0.84615385 0.80769231 0.88       0.96      ]\n",
      "각 폴드의 AUC: [0.80769231 0.88461538 0.73076923 0.76923077 0.76923077 0.76923077\n",
      " 0.84615385 0.80769231 0.88141026 0.96153846]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8228, 0.8321, 0.8225, 0.8228\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0668, 0.0631, 0.0663, 0.0668\n",
      "\n",
      "==============================\n",
      "Cross Validation for RandomForestClassifier(max_depth=3, random_state=101) in fold 10, with seed 101\n",
      "각 폴드의 Recall: [0.88461538 0.80769231 0.76923077 0.73076923 0.76923077 0.73076923\n",
      " 0.88461538 0.84615385 0.83974359 0.92307692]\n",
      "각 폴드의 Precision: [0.88690476 0.80952381 0.79738562 0.74375    0.77575758 0.77083333\n",
      " 0.88690476 0.84615385 0.83974359 0.92857143]\n",
      "각 폴드의 Accuracy: [0.88461538 0.80769231 0.76923077 0.73076923 0.76923077 0.73076923\n",
      " 0.88461538 0.84615385 0.84       0.92      ]\n",
      "각 폴드의 AUC: [0.88461538 0.80769231 0.76923077 0.73076923 0.76923077 0.73076923\n",
      " 0.88461538 0.84615385 0.83974359 0.92307692]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8186, 0.8286, 0.8183, 0.8186\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0642, 0.0565, 0.0637, 0.0642\n",
      "\n",
      "==============================\n",
      "Cross Validation for LogisticRegression(max_iter=1000, random_state=101) in fold 10, with seed 101\n",
      "각 폴드의 Recall: [0.84615385 0.84615385 0.76923077 0.73076923 0.76923077 0.76923077\n",
      " 0.84615385 0.80769231 0.84294872 0.91987179]\n",
      "각 폴드의 Precision: [0.84615385 0.84615385 0.77575758 0.73214286 0.77575758 0.84210526\n",
      " 0.84615385 0.80952381 0.8474026  0.91987179]\n",
      "각 폴드의 Accuracy: [0.84615385 0.84615385 0.76923077 0.73076923 0.76923077 0.76923077\n",
      " 0.84615385 0.80769231 0.84       0.92      ]\n",
      "각 폴드의 AUC: [0.84615385 0.84615385 0.76923077 0.73076923 0.76923077 0.76923077\n",
      " 0.84615385 0.80769231 0.84294872 0.91987179]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8147, 0.8241, 0.8145, 0.8147\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.053, 0.0498, 0.0529, 0.053\n",
      "\n",
      "==============================\n",
      "Cross Validation for GradientBoostingClassifier(learning_rate=0.01, random_state=101) in fold 10, with seed 101\n",
      "각 폴드의 Recall: [0.80769231 0.76923077 0.76923077 0.73076923 0.80769231 0.73076923\n",
      " 0.88461538 0.80769231 0.87820513 0.92307692]\n",
      "각 폴드의 Precision: [0.825      0.77575758 0.79738562 0.74375    0.80952381 0.77083333\n",
      " 0.88690476 0.80952381 0.88311688 0.92857143]\n",
      "각 폴드의 Accuracy: [0.80769231 0.76923077 0.76923077 0.73076923 0.80769231 0.73076923\n",
      " 0.88461538 0.80769231 0.88       0.92      ]\n",
      "각 폴드의 AUC: [0.80769231 0.76923077 0.76923077 0.73076923 0.80769231 0.73076923\n",
      " 0.88461538 0.80769231 0.87820513 0.92307692]\n",
      "평균 Recall, Precision, Accuracy, AUC:\n",
      "0.8109, 0.823, 0.8108, 0.8109\n",
      "표준편차 Recall, Precision, Accuracy, AUC:\n",
      "0.0624, 0.0558, 0.062, 0.0624\n",
      "\n",
      "Model: SVC\n",
      "Recall Precision Accuracy AUC\n",
      "0.8148, 0.8241, 0.8149, 0.8148\n",
      "0.0553, 0.0560, 0.0551, 0.0553\n",
      "==================================================\n",
      "Model: RandomForestClassifier\n",
      "Recall Precision Accuracy AUC\n",
      "0.8203, 0.8304, 0.8201, 0.8203\n",
      "0.0599, 0.0598, 0.0598, 0.0599\n",
      "==================================================\n",
      "Model: LogisticRegression\n",
      "Recall Precision Accuracy AUC\n",
      "0.8109, 0.8205, 0.8110, 0.8109\n",
      "0.0526, 0.0523, 0.0525, 0.0526\n",
      "==================================================\n",
      "Model: GradientBoostingClassifier\n",
      "Recall Precision Accuracy AUC\n",
      "0.8079, 0.8188, 0.8077, 0.8079\n",
      "0.0660, 0.0658, 0.0659, 0.0660\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Total Dataset ( Food + Basic ) \"\"\"\n",
    "\n",
    "results_all_seed = {}\n",
    "\n",
    "n_split = 10\n",
    "for seed in [42, 59, 63, 79, 101]:    \n",
    "    datas = {\"original\":(), \"undersampling\":(), \"oversampling\":()}\n",
    "\n",
    "    datas['undersampling'] = undersampling(no_jilhwan_basic_no_hb, seed=seed)        # eGFR 만을 빼면 성능이 줄어든다..\n",
    "    datas['oversampling'] = oversampling(no_jilhwan_basic_no_hb, seed=seed)\n",
    "    y_tight3_foodsum_train = no_jilhwan_basic_no_hb['onset_tight']\n",
    "    X_tight3_foodsum_train = no_jilhwan_basic_no_hb.drop(['RID', 'onset_tight'], axis=1)\n",
    "    datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "    \n",
    "    results_all_seed[seed] = {}  # 각 시드마다 딕셔너리 생성\n",
    "    \n",
    "    for data in ['undersampling']:\n",
    "        print(f\"For {data} dataset!!!\")\n",
    "        # X_test, y_test = datas['test']\n",
    "        X_train, y_train = datas[data]\n",
    "        \n",
    "        wei_train_scaler = StandardScaler()\n",
    "        X_train = wei_train_scaler.fit_transform(X_train)\n",
    "        # X_test = wei_train_scaler.transform(X_test)\n",
    "        print(f\"{data} dataset loaded and scaled\")\n",
    "        print(f\"X_train :: {X_train.shape}, y_train :: {y_train.shape}\")\n",
    "        \n",
    "        scoring = {\n",
    "        'recall': 'recall_macro',      # recall for each class, then averaged\n",
    "        'precision': 'precision_macro',# precision for each class, then averaged\n",
    "        'accuracy': 'accuracy',        # accuracy\n",
    "        'auc': make_scorer(roc_auc_score, multi_class='ovr')  # AUC 계산 (이진 분류시)\n",
    "        }\n",
    "        \n",
    "        models = (\n",
    "            SVC(kernel='linear', random_state=seed, probability=True),\n",
    "            RandomForestClassifier(random_state=seed, max_depth=3),\n",
    "            LogisticRegression(max_iter=1000, random_state=seed),\n",
    "            GradientBoostingClassifier(random_state=seed, max_depth=3, learning_rate=0.01),\n",
    "            # xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=seed)\n",
    "        )\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            model_name = model.__class__.__name__  # 모델 이름 저장\n",
    "            print(\"=\" * 30)\n",
    "            print(f\"Cross Validation for {model} in fold {n_split}, with seed {seed}\")\n",
    "            # Stratified K-Fold 교차 검증 (K=5)\n",
    "            skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "            results = cross_validate(model, X_train, y_train, cv=skf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            # 결과 출력\n",
    "            print(\"각 폴드의 Recall:\", results['test_recall'])\n",
    "            print(\"각 폴드의 Precision:\", results['test_precision'])\n",
    "            print(\"각 폴드의 Accuracy:\", results['test_accuracy'])\n",
    "            print(\"각 폴드의 AUC:\", results['test_auc'])\n",
    "\n",
    "            # 결과 저장\n",
    "            if model_name not in results_all_seed[seed]:\n",
    "                results_all_seed[seed][model_name] = {\n",
    "                    'recall': [], \n",
    "                    'precision': [], \n",
    "                    'accuracy': [], \n",
    "                    'auc': [],\n",
    "                    'recall_std': [], \n",
    "                    'precision_std': [], \n",
    "                    'accuracy_std': [], \n",
    "                    'auc_std': []\n",
    "                }\n",
    "            results_all_seed[seed][model_name]['recall'].append(np.mean(results['test_recall']))\n",
    "            results_all_seed[seed][model_name]['precision'].append(np.mean(results['test_precision']))\n",
    "            results_all_seed[seed][model_name]['accuracy'].append(np.mean(results['test_accuracy']))\n",
    "            results_all_seed[seed][model_name]['auc'].append(np.mean(results['test_auc']))\n",
    "            results_all_seed[seed][model_name]['recall_std'].append(np.std(results['test_recall']))\n",
    "            results_all_seed[seed][model_name]['precision_std'].append(np.std(results['test_precision']))\n",
    "            results_all_seed[seed][model_name]['accuracy_std'].append(np.std(results['test_accuracy']))\n",
    "            results_all_seed[seed][model_name]['auc_std'].append(np.std(results['test_auc']))\n",
    "            \n",
    "            \n",
    "            # 평균값 계산\n",
    "            print(\"평균 Recall, Precision, Accuracy, AUC:\")\n",
    "            print(f\"{round(np.mean(results['test_recall']), 4)}, {round(np.mean(results['test_precision']),4)}, {round(np.mean(results['test_accuracy']), 4)}, {round(np.mean(results['test_auc']), 4)}\")\n",
    "            print(\"표준편차 Recall, Precision, Accuracy, AUC:\")\n",
    "            print(f\"{round(np.std(results['test_recall']), 4)}, {round(np.std(results['test_precision']),4)}, {round(np.std(results['test_accuracy']), 4)}, {round(np.std(results['test_auc']), 4)}\")\n",
    "            print()\n",
    "            \n",
    "print_cv_results_ML(results_all_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shap value load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['SVC', 'RandomForest', 'LogisticRegression', 'GradientBoosting', 'XGBoost'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(105, 36)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 저장된 shap_values_all_models 불러오기\n",
    "shap_values_save_path = '/home/user19/pnu_ckd/hexa_preprocessing_after95/0911_dl_models/shap_figs/basic_foodadjmean_under_seed42/shap_values_all_models.pk'  # 저장된 경로 지정\n",
    "\n",
    "# pickle을 사용해 파일 불러오기\n",
    "with open(shap_values_save_path, 'rb') as f:\n",
    "    shap_values_all_models_loaded = pickle.load(f)\n",
    "\n",
    "print(shap_values_all_models_loaded.keys())\n",
    "shap_abs_mean = shap_values_all_models_loaded['LogisticRegression'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold shap scale\n",
      "4.286749061609158 1.41187184172254e-06\n",
      "\n",
      "2-th fold shap scale\n",
      "3.747013758829214 5.076857247376431e-07\n",
      "\n",
      "3-th fold shap scale\n",
      "4.303282668124774 1.591530099128552e-05\n",
      "\n",
      "4-th fold shap scale\n",
      "3.5290529379354543 2.8820354409382918e-06\n",
      "\n",
      "5-th fold shap scale\n",
      "3.580234334300884 1.7128125810304588e-06\n",
      "\n",
      "6-th fold shap scale\n",
      "4.982286312347576 4.299012648649807e-06\n",
      "\n",
      "7-th fold shap scale\n",
      "3.4649812373145394 6.755026160410896e-05\n",
      "\n",
      "8-th fold shap scale\n",
      "2.9875469628432634 9.053051306757784e-07\n",
      "\n",
      "9-th fold shap scale\n",
      "4.017277107283533 5.38522143218238e-06\n",
      "\n",
      "10-th fold shap scale\n",
      "4.13459710829984 1.3408297561485307e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{i+1}-th fold shap scale\")\n",
    "    print(np.abs(shap_values_all_models_loaded['LogisticRegression'][i]).max(), np.abs(shap_values_all_models_loaded['LogisticRegression'][i]).min())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold shap scale\n",
      "1.2479852890957288 0.0\n",
      "\n",
      "2-th fold shap scale\n",
      "1.212216992917501 0.0\n",
      "\n",
      "3-th fold shap scale\n",
      "1.4257061903826438 0.0\n",
      "\n",
      "4-th fold shap scale\n",
      "1.2246024438149983 0.0\n",
      "\n",
      "5-th fold shap scale\n",
      "1.1816490861238718 0.0\n",
      "\n",
      "6-th fold shap scale\n",
      "1.282482581213648 0.0\n",
      "\n",
      "7-th fold shap scale\n",
      "1.2406411938535014 0.0\n",
      "\n",
      "8-th fold shap scale\n",
      "1.3375645971275252 0.0\n",
      "\n",
      "9-th fold shap scale\n",
      "1.2465252556032014 0.0\n",
      "\n",
      "10-th fold shap scale\n",
      "1.298005765441243 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{i+1}-th fold shap scale\")\n",
    "    print(np.abs(shap_values_all_models_loaded['GradientBoosting'][i]).max(), np.abs(shap_values_all_models_loaded['GradientBoosting'][i]).min())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold shap scale\n",
      "0.43232088168233 0.0\n",
      "\n",
      "2-th fold shap scale\n",
      "0.4904088140437238 0.0\n",
      "\n",
      "3-th fold shap scale\n",
      "0.5157758828684269 0.0\n",
      "\n",
      "4-th fold shap scale\n",
      "0.4577115588539231 0.0\n",
      "\n",
      "5-th fold shap scale\n",
      "0.38982626043568236 0.0\n",
      "\n",
      "6-th fold shap scale\n",
      "0.4948495550605818 0.0\n",
      "\n",
      "7-th fold shap scale\n",
      "0.4490888622274815 0.0\n",
      "\n",
      "8-th fold shap scale\n",
      "0.40417296415978415 0.0\n",
      "\n",
      "9-th fold shap scale\n",
      "0.4209066783995242 0.0\n",
      "\n",
      "10-th fold shap scale\n",
      "0.5070299408536776 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{i+1}-th fold shap scale\")\n",
    "    print(np.abs(shap_values_all_models_loaded['SVC'][i][1]).max(), np.abs(shap_values_all_models_loaded['SVC'][i][1]).min())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold shap scale\n",
      "0.1587132695610463 0.0\n",
      "[0.00461031 0.07612653 0.10156494 0.01755901 0.01475149 0.00060418\n",
      " 0.00131622 0.00062813 0.00080405 0.00758513 0.         0.00032048\n",
      " 0.03386446 0.01429116 0.00409383 0.00181028 0.02324033 0.00134914\n",
      " 0.00610387 0.00228371 0.00076543 0.00243884 0.00117226 0.00219424\n",
      " 0.000522   0.0007942  0.00242447 0.00037002 0.00115734 0.00150833\n",
      " 0.00260207 0.00061042 0.00032596 0.00109502 0.00597259 0.00346242]\n",
      "\n",
      "2-th fold shap scale\n",
      "0.16300538214938887 1.293182878623055e-07\n",
      "[2.58343151e-03 7.11849080e-02 1.03565634e-01 1.71076449e-02\n",
      " 1.11386901e-02 8.45767539e-04 2.04384754e-03 8.13740509e-04\n",
      " 1.86978166e-03 7.33711092e-03 6.87847928e-05 7.97422812e-04\n",
      " 2.89678703e-02 1.83408810e-02 2.67170763e-03 1.32452150e-03\n",
      " 2.44576532e-02 1.12481287e-03 5.22354139e-03 1.37103208e-03\n",
      " 7.09749599e-04 9.26858193e-04 9.06328362e-04 9.54541071e-04\n",
      " 1.20496955e-03 9.04167786e-04 5.77481085e-03 6.08010061e-04\n",
      " 1.22728031e-03 7.90397647e-04 6.44623133e-04 7.42860043e-04\n",
      " 1.04612714e-03 9.20414489e-04 4.57317458e-03 5.42720707e-03]\n",
      "\n",
      "3-th fold shap scale\n",
      "0.16995851523521982 0.0\n",
      "[0.00195012 0.07756182 0.10089719 0.0165179  0.01574103 0.00146687\n",
      " 0.00146321 0.00148165 0.00212442 0.00898272 0.         0.00101869\n",
      " 0.03250192 0.01349255 0.00305416 0.00090043 0.01978451 0.00067315\n",
      " 0.00547714 0.00164048 0.00087856 0.00135946 0.00089463 0.00064765\n",
      " 0.00124035 0.00099555 0.00262646 0.00119927 0.00109301 0.00087241\n",
      " 0.00231785 0.00056838 0.00102088 0.00062704 0.00793415 0.00528744]\n",
      "\n",
      "4-th fold shap scale\n",
      "0.14917302479771766 4.0790032106499934e-08\n",
      "[0.00383668 0.07553273 0.09751559 0.01536142 0.01677339 0.00091637\n",
      " 0.00353565 0.00082979 0.00177672 0.00582767 0.00017749 0.00143588\n",
      " 0.0306372  0.02003032 0.00533217 0.00177274 0.02907998 0.00050845\n",
      " 0.00562013 0.0015338  0.00076545 0.00084753 0.00044885 0.00077206\n",
      " 0.00035316 0.00056708 0.00201027 0.00043228 0.0003304  0.00068107\n",
      " 0.00192812 0.00082057 0.00127623 0.00032151 0.00626868 0.00436967]\n",
      "\n",
      "5-th fold shap scale\n",
      "0.1482552201450673 1.0370478157593006e-06\n",
      "[0.00949732 0.07464558 0.10202325 0.01098294 0.01251758 0.00036039\n",
      " 0.00337426 0.00160387 0.00168829 0.00855735 0.00050249 0.00021767\n",
      " 0.02828703 0.01778661 0.00529117 0.00206075 0.02255234 0.00107345\n",
      " 0.00619226 0.00065148 0.00123407 0.00105582 0.00105899 0.00066422\n",
      " 0.00055348 0.00092359 0.0018769  0.00042678 0.00080571 0.00134288\n",
      " 0.00327091 0.00069928 0.000165   0.00109938 0.00568035 0.00406133]\n",
      "\n",
      "6-th fold shap scale\n",
      "0.1569658190934475 0.0\n",
      "[0.00570085 0.07660395 0.10104522 0.01456426 0.01330929 0.00143972\n",
      " 0.00189013 0.00255205 0.00205417 0.00828814 0.         0.00049277\n",
      " 0.03099727 0.02057101 0.00456555 0.00187525 0.02100527 0.0004882\n",
      " 0.0027089  0.00064236 0.00131594 0.00080179 0.00066491 0.00151964\n",
      " 0.00157418 0.00070165 0.00186568 0.00058189 0.00116292 0.00035815\n",
      " 0.00199295 0.0005018  0.00063586 0.00076787 0.00453883 0.00503298]\n",
      "\n",
      "7-th fold shap scale\n",
      "0.15614003050468908 0.0\n",
      "[0.00249792 0.07856031 0.10896025 0.01686693 0.01328298 0.00138194\n",
      " 0.00336754 0.00139283 0.0027744  0.00760529 0.         0.00022519\n",
      " 0.02382907 0.01837302 0.00477323 0.00157546 0.01938046 0.00291563\n",
      " 0.00647145 0.00074325 0.00121237 0.00050765 0.00082274 0.00083643\n",
      " 0.00081881 0.00117949 0.00234415 0.00045157 0.00033796 0.00039865\n",
      " 0.00139878 0.00107639 0.00063889 0.00068726 0.00400127 0.00363466]\n",
      "\n",
      "8-th fold shap scale\n",
      "0.1431183465713287 0.0\n",
      "[0.00889159 0.07464027 0.08918065 0.01762172 0.01011281 0.00104893\n",
      " 0.00257602 0.00153375 0.00251132 0.00732801 0.         0.00265801\n",
      " 0.03192676 0.01453831 0.00480292 0.00272902 0.02423194 0.00037498\n",
      " 0.00450693 0.00065918 0.00143532 0.00099346 0.00067761 0.0011263\n",
      " 0.00063327 0.00088953 0.00209147 0.00142599 0.00065259 0.00297119\n",
      " 0.00153652 0.00065645 0.00046719 0.00113364 0.00162521 0.00317085]\n",
      "\n",
      "9-th fold shap scale\n",
      "0.15425418968101687 9.57363935829577e-07\n",
      "[0.00330018 0.0776808  0.10046394 0.0195531  0.01108265 0.00118795\n",
      " 0.00263277 0.00097864 0.00542423 0.00811536 0.00011728 0.00114789\n",
      " 0.03315348 0.01619987 0.00498218 0.00177083 0.02002077 0.00100937\n",
      " 0.00800268 0.00158362 0.00059035 0.00092577 0.00072718 0.00081019\n",
      " 0.00040879 0.00066701 0.00311147 0.00029376 0.00103542 0.00179886\n",
      " 0.0041467  0.00135567 0.0006633  0.00088463 0.00283105 0.00427344]\n",
      "\n",
      "10-th fold shap scale\n",
      "0.1676225358807182 9.071859042160706e-07\n",
      "[0.00853489 0.07772382 0.09570349 0.01707654 0.0139417  0.00069324\n",
      " 0.00295737 0.00114955 0.00385286 0.00746345 0.00024457 0.00059499\n",
      " 0.03466333 0.01440601 0.00366478 0.00256808 0.02417132 0.00118778\n",
      " 0.00352613 0.00068812 0.00084918 0.00106335 0.00064924 0.001811\n",
      " 0.0011037  0.00089101 0.00241795 0.00058178 0.00054427 0.00085511\n",
      " 0.00157648 0.0006727  0.00081726 0.00055681 0.00421622 0.0051324 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{i+1}-th fold shap scale\")\n",
    "    print(np.abs(shap_values_all_models_loaded['RandomForest'][i][1]).max(), np.abs(shap_values_all_models_loaded['RandomForest'][i][1]).min())\n",
    "    print(np.abs(shap_values_all_models_loaded['RandomForest'][i][1]).mean(axis=0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 36)\n",
      "(105, 36)\n",
      "(105, 36)\n",
      "(105, 36)\n",
      "(105, 36)\n",
      "(105, 36)\n",
      "(105, 36)\n",
      "(105, 36)\n",
      "(105, 36)\n",
      "(105, 36)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(shap_values_all_models_loaded['SVC'][i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 36)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values_all_models_loaded['SVC'][0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian model 성능확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57231, 38) (57231, 17) (57231, 23)\n"
     ]
    }
   ],
   "source": [
    "print(total.shape, basic.shape, food.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For oversampling dataset!!!\n",
      "oversampling dataset loaded and scaled\n",
      "X_train :: (113308, 36), y_train :: (113308,)\n",
      "Training LogisticRegression model on fold 1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model on fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel fitted!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m y_tight3_foodsum_test \u001b[38;5;241m=\u001b[39m b[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monset_tight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1303\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1301\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1303\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:452\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    448\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[1;32m    449\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    450\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    451\u001b[0m ]\n\u001b[0;32m--> 452\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    461\u001b[0m     solver,\n\u001b[1;32m    462\u001b[0m     opt_res,\n\u001b[1;32m    463\u001b[0m     max_iter,\n\u001b[1;32m    464\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    465\u001b[0m )\n\u001b[1;32m    466\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/sklearn/linear_model/_linear_loss.py:279\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[0;32m--> 279\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    286\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_penalty(weights, l2_reg_strength)\n",
      "File \u001b[0;32m/opt/conda/envs/ckd2/lib/python3.8/site-packages/sklearn/_loss/loss.py:253\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[0;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    251\u001b[0m     gradient_out \u001b[38;5;241m=\u001b[39m gradient_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Total Dataset ( Food + Basic ) \"\"\"\n",
    "\n",
    "results_all_seed = {}\n",
    "\n",
    "# total, basic, food\n",
    "a, b = divide_testset(total, ratio=0.1)\n",
    "\n",
    "n_split = 10\n",
    "for seed in [42, 59, 63, 79, 101]:    \n",
    "# for seed in [42]:\n",
    "    datas = {\"original\":(), \"undersampling\":(), \"oversampling\":()}\n",
    "\n",
    "    datas['undersampling'] = undersampling(a, seed=seed)        # eGFR 만을 빼면 성능이 줄어든다..\n",
    "    datas['oversampling'] = oversampling(a, seed=seed)\n",
    "    y_tight3_foodsum_train = a['onset_tight']\n",
    "    X_tight3_foodsum_train = a.drop(['RID', 'onset_tight'], axis=1)\n",
    "    datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "    \n",
    "    results_all_seed[seed] = {}  # 각 시드마다 딕셔너리 생성\n",
    "    \n",
    "    for data in ['oversampling']:\n",
    "        print(f\"For {data} dataset!!!\")\n",
    "        # X_test, y_test = datas['test']\n",
    "        X_train, y_train = datas[data]\n",
    "        \n",
    "        # wei_train_scaler = StandardScaler()\n",
    "        # X_train = wei_train_scaler.fit_transform(X_train)\n",
    "        # X_test = wei_train_scaler.transform(X_test)\n",
    "        print(f\"{data} dataset loaded and scaled\")\n",
    "        print(f\"X_train :: {X_train.shape}, y_train :: {y_train.shape}\")\n",
    "        \n",
    "        models = (\n",
    "            # SVC(kernel='linear', random_state=seed, probability=True),\n",
    "            # RandomForestClassifier(random_state=seed, max_depth=3),\n",
    "            LogisticRegression(max_iter=1000, random_state=seed),\n",
    "            # GradientBoostingClassifier(random_state=seed, max_depth=3, learning_rate=0.01),\n",
    "            # GaussianNB(var_smoothing=1e-7),     # 1e-11, 1e-10, 1e-9, 1e-8, 1e-7\n",
    "            # xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=seed)\n",
    "        )\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "            \n",
    "        for fold_num, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "            # Train/Test 데이터 분할\n",
    "            X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            for model in models:\n",
    "                print(f\"Training {model.__class__.__name__} model on fold {fold_num}...\")\n",
    "                model.fit(X_fold_train, y_fold_train)\n",
    "                print(f\"Model fitted!!\")\n",
    "        \n",
    "        y_tight3_foodsum_test = b['onset_tight']\n",
    "        X_tight3_foodsum_test = b.drop(['RID', 'onset_tight'], axis=1)\n",
    "        \n",
    "        for trained_model in models:\n",
    "            print(\"=\"*30)\n",
    "            print(f\"Predicting {model.__class__.__name__}!!\")\n",
    "            preds = trained_model.predict(X_tight3_foodsum_test)\n",
    "            model_probabilities = trained_model.predict_proba(X_tight3_foodsum_test)[:, 1]\n",
    "            print(\"=\" * 30)\n",
    "            print(seed)\n",
    "            get_results(y_tight3_foodsum_test, np.array(preds), model_probabilities)\n",
    "            print(\"=\" * 30)\n",
    "            \n",
    "# print_cv_results_ML(results_all_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_seed = {}\n",
    "\n",
    "# total, basic, food\n",
    "a, b = divide_testset(total, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For original dataset!!!\n",
      "(57127, 21) (57127,)\n",
      "original dataset loaded and scaled\n",
      "X_train :: (57127, 21), y_train :: (57127,)\n",
      "Model fitted!!\n",
      "==============================\n",
      "Predicting GaussianNB!!\n",
      "==============================\n",
      "42\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.5411\n",
      "==============================\n",
      "For original dataset!!!\n",
      "(57127, 21) (57127,)\n",
      "original dataset loaded and scaled\n",
      "X_train :: (57127, 21), y_train :: (57127,)\n",
      "Model fitted!!\n",
      "==============================\n",
      "Predicting GaussianNB!!\n",
      "==============================\n",
      "59\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.5411\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in long_scalars\n",
      "invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For original dataset!!!\n",
      "(57127, 21) (57127,)\n",
      "original dataset loaded and scaled\n",
      "X_train :: (57127, 21), y_train :: (57127,)\n",
      "Model fitted!!\n",
      "==============================\n",
      "Predicting GaussianNB!!\n",
      "==============================\n",
      "63\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.5411\n",
      "==============================\n",
      "For original dataset!!!\n",
      "(57127, 21) (57127,)\n",
      "original dataset loaded and scaled\n",
      "X_train :: (57127, 21), y_train :: (57127,)\n",
      "Model fitted!!\n",
      "==============================\n",
      "Predicting GaussianNB!!\n",
      "==============================\n",
      "79\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.5411\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in long_scalars\n",
      "invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For original dataset!!!\n",
      "(57127, 21) (57127,)\n",
      "original dataset loaded and scaled\n",
      "X_train :: (57127, 21), y_train :: (57127,)\n",
      "Model fitted!!\n",
      "==============================\n",
      "Predicting GaussianNB!!\n",
      "==============================\n",
      "101\n",
      "[[52  0]\n",
      " [52  0]]\n",
      "Recall \t Precision \t Acc \t AUC\n",
      "0.0 nan 0.5 0.5411\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Total Dataset ( Food + Basic ) \"\"\"\n",
    "\n",
    "# results_all_seed = {}\n",
    "\n",
    "# # total, basic, food\n",
    "# a, b = divide_testset(total, ratio=0.1)\n",
    "\n",
    "n_split = 10\n",
    "for seed in [42, 59, 63, 79, 101]:    \n",
    "# for seed in [42]:    \n",
    "    datas = {\"original\":(), \"undersampling\":(), \"oversampling\":()}\n",
    "\n",
    "    datas['undersampling'] = undersampling(a, seed=seed)        # eGFR 만을 빼면 성능이 줄어든다..\n",
    "    datas['oversampling'] = oversampling(a, seed=seed)\n",
    "    y_tight3_foodsum_train = a['onset_tight']\n",
    "    X_tight3_foodsum_train = a.drop(['RID', 'onset_tight'], axis=1)\n",
    "    datas['original'] = (X_tight3_foodsum_train, y_tight3_foodsum_train)\n",
    "    \n",
    "    results_all_seed[seed] = {}  # 각 시드마다 딕셔너리 생성\n",
    "    \n",
    "    for data in ['original']:\n",
    "        print(f\"For {data} dataset!!!\")\n",
    "        # X_test, y_test = datas['test']\n",
    "        X_train, y_train = datas[data]\n",
    "        print(X_train.shape, y_train.shape)\n",
    "        \n",
    "        # wei_train_scaler = StandardScaler()\n",
    "        # X_train = wei_train_scaler.fit_transform(X_train)\n",
    "        # X_test = wei_train_scaler.transform(X_test)\n",
    "        print(f\"{data} dataset loaded and scaled\")\n",
    "        print(f\"X_train :: {X_train.shape}, y_train :: {y_train.shape}\")\n",
    "        \n",
    "        models = (\n",
    "            # SVC(kernel='linear', random_state=seed, probability=True),\n",
    "            # RandomForestClassifier(random_state=seed, max_depth=3),\n",
    "            # LogisticRegression(max_iter=1000, random_state=seed),\n",
    "            GradientBoostingClassifier(random_state=seed, max_depth=3, learning_rate=0.01),\n",
    "            # GaussianNB(var_smoothing=1e-7),     # 1e-11, 1e-10, 1e-9, 1e-8, 1e-7\n",
    "            # xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=seed)\n",
    "        )\n",
    "        \n",
    "        # skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "            \n",
    "        # for fold_num, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        #     # Train/Test 데이터 분할\n",
    "        #     X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        #     y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        #     for model in models:\n",
    "        #         print(f\"Training {model.__class__.__name__} model on fold {fold_num}...\")\n",
    "        #         model.fit(X_fold_train, y_fold_train)\n",
    "        #         print(f\"Model fitted!!\")\n",
    "        \n",
    "        model = models[0]\n",
    "        model.fit(X_train, y_train)\n",
    "        print(f\"Model fitted!!\")\n",
    "        \n",
    "        y_tight3_foodsum_test = b['onset_tight']\n",
    "        X_tight3_foodsum_test = b.drop(['RID', 'onset_tight'], axis=1)\n",
    "        \n",
    "        for trained_model in models:\n",
    "            print(\"=\"*30)\n",
    "            print(f\"Predicting {model.__class__.__name__}!!\")\n",
    "            preds = trained_model.predict(X_tight3_foodsum_test)\n",
    "            model_probabilities = trained_model.predict_proba(X_tight3_foodsum_test)[:, 1]\n",
    "            print(\"=\" * 30)\n",
    "            print(seed)\n",
    "            get_results(y_tight3_foodsum_test, np.array(preds), model_probabilities)\n",
    "            print(\"=\" * 30)\n",
    "            \n",
    "# print_cv_results_ML(results_all_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
